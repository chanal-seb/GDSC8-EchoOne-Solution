{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aec46e5-52bf-4fcd-b868-63fbab04bf54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: strands-agents[mistral] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.39)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.39)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (0.17.0)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.38.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.12.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Requirement already satisfied: mistralai>=1.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.9.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.4.3)\n",
      "Requirement already satisfied: httpx>=0.27.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.59b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (0.59b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (0.59b0)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (0.2.2)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (2.2.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fec45b-aa58-44fb-827c-0175956f2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "\tchat_with_persona,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract,\n",
    "    compute_stat_for_multi_items\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo, JobPersonaInfo, LoacationNameRefator\n",
    "from src.models.activity_domain_info import ActivityDomainInfo, ListOfActivityDomains\n",
    "from src.models.skill_domain_info import SkillDomainInfo, ListSkillsDomains\n",
    "from src.prompts.persona_extraction_prompt import (\n",
    "    PERSONA_JOB_EXTRACTION_PROMPT,\n",
    "    PERSONA_INITIAL_EXTRACTION_PROMPT,\n",
    "    PERSONA_EXTEND_SKILL_DOMAIN_PROMPT,\n",
    "    PERSONA_SKILL_DOMAINS_CLASSIFICATION_PROMPT,\n",
    "    PERSONA_ACTIVITY_DOMAINS_CLASSIFICATION_PROMPT,\n",
    "    LOCATION_NAME_REFACTORING\n",
    ")\n",
    "from src.prompts.interview_prompt import(\n",
    "    JOB_INTERVIEW_QUALITY_CHECK_PROMPT\n",
    ")\n",
    "from src.models.interview_info import(\n",
    "    InverviewQualityInfo\n",
    ")\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ùå No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"‚úÖ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e41af00-3ed7-4d1f-8bd0-b16446186248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_INTERVIEWS_DIR = Path('../data_interviews')\n",
    "DATA_ACTIVITIES_DOMAINS_DIR = Path('../data_activities_domains')\n",
    "DATA_SKILLS_DOMAINS_DIR = Path('../data_skills_domains')\n",
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b5960a-d12b-4af8-b533-ba27fed5f106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "interview_data_version version: v8\n",
      "activity_domains_version version: v4\n",
      "personas_info_data_version version: v14\n",
      "skill_domains_version version: v3\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "interview_data_version = config[\"interview_data_version\"]\n",
    "print(f\"interview_data_version version: {interview_data_version}\")\n",
    "\n",
    "activity_domains_version = config[\"activity_domains_version\"]\n",
    "print(f\"activity_domains_version version: {activity_domains_version}\")\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "skill_domains_version = config[\"skill_domains_version\"]\n",
    "print(f\"skill_domains_version version: {skill_domains_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06806ca7-5fe3-4fb0-a851-57938ac909c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jobs domains map data\n",
    "filename = f\"map_clusters_jobs_{job_data_version}.json\"\n",
    "save_path = DATA_JOBS_DIR / filename\n",
    "jobs_map = read_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb17430b-2038-4b5f-9397-fe62d8395a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 12 skills domains\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load skills domains map data\n",
    "filename = f\"final_map_clusters_trainings_{training_data_version}.json\"\n",
    "save_path = DATA_TRAININGS_DIR / filename\n",
    "trainings_map = read_json(save_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(trainings_map)} skills domains\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f7e10af-6bfa-4ce1-b458-7e76a7c44277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interviews\n",
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "initial_interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4158063d-9c35-4c5b-b063-fb18ec03cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interviews\n",
    "filename = f\"job_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "job_extension_interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4f22bba-8ccc-49f8-b6b5-8675a90e87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 personas\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Personas data\n",
    "filename = f\"recommandation_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "\n",
    "initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in initial_personas_data.items()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(personas)} personas\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519814f-b756-4201-a435-a8bd4239baeb",
   "metadata": {},
   "source": [
    "# Extract Persona Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af10406c-e2f9-4f05-a4d5-e84fcebf8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_persona_info(\n",
    "    activities_domains: str,\n",
    "    conversation: List[str],\n",
    "    previous_goal: str,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> JobPersonaInfo:\n",
    "    \"\"\"Extract persona info from conversation using Persona Extraction Agent\"\"\"\n",
    "    text = '\\n'.join(conversation)\n",
    "    #print(text)\n",
    "\n",
    "    prompt = PERSONA_JOB_EXTRACTION_PROMPT.format(\n",
    "        activities_domains=activities_domains,\n",
    "        conversation=text,\n",
    "        previous_goal=previous_goal\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(prompt)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # return None\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=JobPersonaInfo, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e945ced-292e-4a45-9173-8ee331b27eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START CLASSIFICATION LOOP\n",
      "ITERATE CLASSIFICATION LOOP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 17/100 [00:26<02:09,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√©cn√≥logo renamed in Tecn√≥logo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [02:25<00:24,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_089 : ['Cultural Information and Records Management'] not in domains list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:41<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "MAX_LOOPS = 1\n",
    "cache_period = 5\n",
    "\n",
    "# Prepare personas info\n",
    "filename = f\"job_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "new_items_processed = 0\n",
    "\n",
    "print(\"START CLASSIFICATION LOOP\")\n",
    "for i in range(MAX_LOOPS):\n",
    "    print(\"ITERATE CLASSIFICATION LOOP\")\n",
    "\n",
    "    new_personas_processed = 0\n",
    "\n",
    "    for persona_id in tqdm(initial_personas_data):\n",
    "        if persona_id in personas_data:\n",
    "            continue\n",
    "\n",
    "        if persona_id not in job_extension_interviews:\n",
    "            personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "            continue\n",
    "\n",
    "        #if personas[persona_id].recommendation_type != 'jobs_trainings':\n",
    "        #    personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "        #    continue\n",
    "\n",
    "        new_personas_processed += 1\n",
    "\n",
    "        conversation = initial_interviews[persona_id]['interview']\n",
    "        conversation += job_extension_interviews[persona_id]['interview']\n",
    "\n",
    "        persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "\n",
    "        domains_str = \"\"\n",
    "        for domain in jobs_map:\n",
    "            domains_str += f\"- {domain} : {jobs_map[domain]['description']}\" + \"\\n\"\n",
    "\n",
    "        print_prompt = False\n",
    "        # if new_personas_processed == 1:\n",
    "        #    print_prompt = True\n",
    "\n",
    "        result = extract_job_persona_info(\n",
    "            domains_str,\n",
    "            conversation,\n",
    "            persona_data_dict['goals'],\n",
    "            model=\"mistral-medium-latest\",\n",
    "            print_prompt=print_prompt\n",
    "        )\n",
    "\n",
    "        if len(result.target_domains) == 0:\n",
    "            print(f\"Activity domains empty : for {persona_id}\")\n",
    "            continue\n",
    "\n",
    "        domain_issue = False\n",
    "        for domain in result.target_domains:\n",
    "            if domain not in jobs_map:\n",
    "                domain_issue = True\n",
    "                print(f\"{persona_id} : {result.target_domains} not in domains list\")\n",
    "\n",
    "        if domain_issue is True:\n",
    "            continue\n",
    "\n",
    "        persona_data_dict['open_to_relocate_for_work'] = result.open_to_relocate_for_work\n",
    "        persona_data_dict['work_type_preference'] = result.work_type_preference\n",
    "        persona_data_dict['education_level'] = result.education_level\n",
    "        persona_data_dict['years_of_experience'] = result.years_of_experience\n",
    "        persona_data_dict['languages'] = result.languages\n",
    "        persona_data_dict['goals'] = result.goals\n",
    "        persona_data_dict['target_domains'] = result.target_domains\n",
    "\n",
    "        if persona_data_dict['education_level'] == 'T√©cn√≥logo':\n",
    "            print('T√©cn√≥logo renamed in Tecn√≥logo')\n",
    "            persona_data_dict['education_level'] = 'Tecn√≥logo'\n",
    "\n",
    "        personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "\n",
    "        # Save every 5 personas\n",
    "        if new_personas_processed % 5 == 0:\n",
    "            save_json(personas_save_path, personas_data)\n",
    "\n",
    "        # Show cost update every 20 personas\n",
    "        # if new_personas_processed > 0 and new_personas_processed % 20 == 0:\n",
    "        #     print(f\"\\nüí∞ Cost update after {new_personas_processed} new personas:\")\n",
    "        #     print_cost_summary()\n",
    "        #     print()\n",
    "\n",
    "        #if new_personas_processed > 0:break\n",
    "\n",
    "save_json(personas_save_path, personas_data)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "# personas = {\n",
    "#     pid: PersonaInfo.model_validate_json(data)\n",
    "#     for pid, data in persona_infos.items()\n",
    "# }\n",
    "\n",
    "# print(f\"\\n‚úÖ Interviewed {len(personas)} personas total ({new_personas_processed} new)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47ef87-4739-4217-8b24-662e73336ac4",
   "metadata": {},
   "source": [
    "# Clean location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "931da2de-d26f-4b66-b6e9-c26a615fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_name_refactoring(\n",
    "    location: str,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> LoacationNameRefator:\n",
    "\n",
    "    prompt = LOCATION_NAME_REFACTORING.format(\n",
    "        location=location\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(prompt)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # return None\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=LoacationNameRefator, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e40f4b9-d6c0-4054-925e-06c8597f47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"job_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "initial_personas_data = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e6016a-7924-4dc3-87e2-692c25cc534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START CLASSIFICATION LOOP\n",
      "ITERATE CLASSIFICATION LOOP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 4/99 [00:07<02:49,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio => Rio de Janeiro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 9/99 [00:14<02:14,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio => Rio de Janeiro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 12/99 [00:35<05:38,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio => Rio de Janeiro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñã       | 26/99 [00:59<02:15,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fortaleza, Brazil => Fortaleza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 51/99 [01:40<01:31,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recife, Brazil => Recife\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 56/99 [01:49<01:11,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio => Rio de Janeiro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/99 [02:37<00:25,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvador, Brazil => Salvador\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [03:02<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio => Rio de Janeiro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LOOPS = 1\n",
    "cache_period = 5\n",
    "\n",
    "# Prepare personas info\n",
    "filename = f\"final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "print(\"START CLASSIFICATION LOOP\")\n",
    "for i in range(MAX_LOOPS):\n",
    "    print(\"ITERATE CLASSIFICATION LOOP\")\n",
    "\n",
    "    new_items_processed = 0\n",
    "    for persona_id in tqdm(initial_personas_data):\n",
    "        new_items_processed += 1\n",
    "        personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "\n",
    "        persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "        result = location_name_refactoring(\n",
    "            persona_data_dict['location'],\n",
    "            model=\"mistral-medium-latest\",\n",
    "            print_prompt=False\n",
    "        )\n",
    "        \n",
    "        if result.renaming_needed is True:\n",
    "            print(f\"{persona_data_dict['location']} => {result.name}\")\n",
    "            persona_data_dict['location'] = result.name\n",
    "            personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "            \n",
    "        if new_items_processed % 5 == 0:\n",
    "            save_json(personas_save_path, personas_data)\n",
    "            \n",
    "save_json(personas_save_path, personas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b51c5-136e-4145-abc5-f1674249433d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93855-4399-4ff7-881e-0bf1f65dc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    persona_id = \"persona_018\"\n",
    "    persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "\n",
    "    conversation = initial_interviews[persona_id]['interview']\n",
    "    conversation += job_extension_interviews[persona_id]['interview']\n",
    "\n",
    "    domains_str = \"\"\n",
    "    for domain in jobs_map:\n",
    "        domains_str += f\"- {domain} : {jobs_map[domain]['description']}\" + \"\\n\"\n",
    "\n",
    "    result = extract_job_persona_info(\n",
    "        domains_str,\n",
    "        conversation,\n",
    "        persona_data_dict['goals'],\n",
    "        model=\"mistral-medium-latest\",\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    persona_data_dict['open_to_relocate_for_work'] = result.open_to_relocate_for_work\n",
    "    persona_data_dict['work_type_preference'] = result.work_type_preference\n",
    "    persona_data_dict['education_level'] = result.education_level\n",
    "    persona_data_dict['years_of_experience'] = result.years_of_experience\n",
    "    persona_data_dict['languages'] = result.languages\n",
    "    persona_data_dict['goals'] = result.goals\n",
    "    persona_data_dict['target_domains'] = result.target_domains\n",
    "\n",
    "    print(persona_data_dict)\n",
    "\n",
    "    # print(persona_info)\n",
    "\n",
    "    # if(persona_info.recommendation_type != \"awareness\"):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbfed8-43f4-4f7d-a9f5-1ce4d1987fc7",
   "metadata": {},
   "source": [
    "# Check quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58ba25b9-e955-4fbc-af91-53ebc5b6ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "persona_infos = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9453cf-198d-49ee-9d3b-77b84ac5c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interview_quality(\n",
    "    interview: str,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt: bool = False\n",
    ") -> InverviewQualityInfo:\n",
    "    prompt = JOB_INTERVIEW_QUALITY_CHECK_PROMPT.format(\n",
    "        interview=interview\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=InverviewQualityInfo, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11043a72-01b4-4884-9beb-c2404c6b0815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"quality_level\": \"OK\", \"rationale\": \"The interview covers all required information: willingness to relocate, education level, years of experience, languages spoken, career goals, and target domains. No information is missing, and there is no indication of hallucinated information by the assistant.\"}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    persona_id = 'persona_026'\n",
    "\n",
    "    filename = f\"quality_job_interviews_{interview_data_version}.json\"\n",
    "    save_path = DATA_INTERVIEWS_DIR / filename\n",
    "    quality_interviews = read_json(save_path)\n",
    "\n",
    "    persona_data_dict = json.loads(persona_infos[persona_id])\n",
    "    if persona_data_dict['recommendation_type'] != 'jobs_trainings':\n",
    "        print('ISSUE')\n",
    "    elif persona_id in quality_interviews:\n",
    "        quality = json.loads(quality_interviews[persona_id])\n",
    "        if quality['quality_level'] == 'OK':\n",
    "            print('ISSUE')\n",
    "        else:\n",
    "            interview = job_extension_interviews[persona_id]['interview']\n",
    "            \n",
    "            interview_str = \"\\n\".join(interview)\n",
    "            # print(interview_str)\n",
    "            \n",
    "            quality = check_interview_quality(interview_str, model='mistral-medium-latest', print_prompt=False)\n",
    "            quality_str = json.dumps(quality.model_dump(), ensure_ascii=False)\n",
    "            \n",
    "            quality_interviews[persona_id] = quality_str\n",
    "            \n",
    "            save_json(save_path, quality_interviews)            \n",
    "\n",
    "            print(quality_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2ac711-b7a8-4e22-8558-52cf394463ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [02:10<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "cache_period = 5\n",
    "\n",
    "filename = f\"quality_job_interviews_{interview_data_version}.json\"\n",
    "save_path = DATA_INTERVIEWS_DIR / filename\n",
    "if not save_path.exists():\n",
    "    save_json(save_path, {})\n",
    "quality_interviews = read_json(save_path)\n",
    "\n",
    "\n",
    "new_items_processed = 0\n",
    "for persona_id in tqdm(job_extension_interviews):\n",
    "    persona_data_dict = json.loads(persona_infos[persona_id])\n",
    "    if persona_data_dict['recommendation_type'] != 'jobs_trainings':\n",
    "        continue\n",
    "\n",
    "    if persona_id in quality_interviews:\n",
    "        quality = json.loads(quality_interviews[persona_id])\n",
    "        if quality['quality_level'] == 'OK':\n",
    "            continue\n",
    "\n",
    "    new_items_processed = new_items_processed + 1\n",
    "\n",
    "    interview = job_extension_interviews[persona_id]['interview']\n",
    "\n",
    "    interview_str = \"\\n\".join(interview)\n",
    "    # print(interview_str)\n",
    "\n",
    "    quality = check_interview_quality(interview_str, model='mistral-medium-latest', print_prompt=False)\n",
    "    quality_str = json.dumps(quality.model_dump(), ensure_ascii=False)\n",
    "\n",
    "    quality_interviews[persona_id] = quality_str\n",
    "\n",
    "    if new_items_processed % cache_period == 0:\n",
    "        save_json(save_path, quality_interviews)\n",
    "\n",
    "    #if new_items_processed > 0:break\n",
    "\n",
    "save_json(save_path, quality_interviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f39c-c11e-467e-98bc-95a1485637c5",
   "metadata": {},
   "source": [
    "# Redo Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e45f70-3478-499e-818c-494ffd5d35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "From the interview conversation, extract the following fields and return a structured profile with these fields:\n",
      "- Open to relocate for work: Whether they've mentioned being willing to move for work (true/false)\n",
      "- Work type preference: Their preferred work arrangement (onsite, remote, or hybrid)\n",
      "- Education level: Their highest completed education level. PICK EXACT LABEL NAME FROM  : Ensino Fundamental, Ensino M√©dio, T√©cnico, Tecn√≥logo, Gradua√ß√£o, Bacharelado, Licenciatura, P√≥s-gradua√ß√£o, Especializa√ß√£o, Mestrado, MBA, Doutorado). IMPORTANT reuse exact same labels !\n",
      "- Years of experience: Their professional experience in years (as an integer)\n",
      "- Languages: Languages they speak\n",
      "    - Use strict formating for language name:\n",
      "        - use english term\n",
      "        - starting with Capital letter, the rest in lowercase letter\n",
      "        - no location attribute (like Brasilian), just one word\n",
      "- Goals: Their stated career or learning objectives.\n",
      "    - Knowing previous stated goal by user was : Work in shows\n",
      "    - Make sure to capture all meaningful informations that will help understand acticity domain or skills domain in relation to candidate profile or goal.\n",
      "- Target domains: Professional domains/sectors they're interested in.\n",
      "    - It shall be from **ACTIVITIES DOMAINS LIST**\n",
      "    - It shall use exact same names (Do not change any letter or caps)\n",
      "    - If no match at all use UNKNOWN as single domain name\n",
      "\n",
      "For any information not explicitly mentioned:\n",
      "- If education level isn't specified, use \"unknown\" rather than guessing\n",
      "\n",
      "Be precise and only extract information that's actually present or can be reasonably inferred. Don't invent details.\n",
      "\n",
      "ACTIVITIES DOMAINS LIST:\n",
      "- Financial Operations And Compliance Management : Managing financial activities, ensuring regulatory compliance, and supporting risk management in finance.\n",
      "- Legal Research And Procedural Support : Conducting legal research, drafting documents, and supporting procedural compliance and case management.\n",
      "- Insurance Operations And Compliance Management : Managing insurance processes, ensuring compliance, assessing risks, and handling client services.\n",
      "- Maritime And Port Operations Management : Managing and coordinating vessel, terminal, and cargo operations in maritime and port environments.\n",
      "- Industrial Production And Live Event Technical Operations : Encompasses equipment design, maintenance, and coordination in industrial and live event settings.\n",
      "- Health Safety And Environmental Compliance : Ensuring workplace safety, regulatory compliance, and environmental health in industrial settings.\n",
      "- Procurement And Supplier Management : Managing supplier relationships and optimizing procurement processes for efficiency and cost reduction.\n",
      "- Electrical And Electronics Systems Engineering : Focuses on electrical and electronic systems design, maintenance, analysis, and project coordination.\n",
      "- Community Engagement And Cultural Development : Focuses on creating and managing social, educational, and cultural community programs.\n",
      "- Tourism And Hospitality Guest Services : Focuses on enhancing guest experiences and managing tourism-related services and operations.\n",
      "- Visual Arts And Studio Production : Creating, managing, and coordinating visual arts and studio production activities.\n",
      "- Cultural Information And Records Management : Managing and preserving cultural records, ensuring accessibility, compliance, and documentation standards.\n",
      "- Design Research And Innovation Management : Focuses on user research, product development, and managing innovation projects in design.\n",
      "- Fiber And Paper Manufacturing Operations : Covers production, quality control, and sustainable practices in fiber and paper industries.\n",
      "- Food Production Quality And Safety Management : Ensuring quality and safety standards in food production and development processes.\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Assistant: Hello, I'm here to helping you navigate your career path.\n",
      "\n",
      "User: Hi‚Ä¶ I want to work in shows, but‚Ä¶ slowly.\n",
      "Assistant: Great ! Do you want to work in Brazil or abroad ?\n",
      "Also, are you looking for a job, training, or information about working in shows?\n",
      "User: Here‚Ä¶ in Salvador. I need a job. Not training.\n",
      "Assistant: How old are you ? It's important to provide the best guidance. If you don't want to tell me your age, just tell me if you are under 16.\n",
      "User: I‚Äôm 18‚Ä¶\n",
      "Assistant: In a single sentence, which domains you are interested in to find a job from follwing list:\n",
      "- Financial Operations And Compliance Management : Managing financial activities, ensuring regulatory compliance, and supporting risk management in finance.\n",
      "- Legal Research And Procedural Support : Conducting legal research, drafting documents, and supporting procedural compliance and case management.\n",
      "- Insurance Operations And Compliance Management : Managing insurance processes, ensuring compliance, assessing risks, and handling client services.\n",
      "- Maritime And Port Operations Management : Managing and coordinating vessel, terminal, and cargo operations in maritime and port environments.\n",
      "- Industrial Production And Live Event Technical Operations : Encompasses equipment design, maintenance, and coordination in industrial and live event settings.\n",
      "- Health Safety And Environmental Compliance : Ensuring workplace safety, regulatory compliance, and environmental health in industrial settings.\n",
      "- Procurement And Supplier Management : Managing supplier relationships and optimizing procurement processes for efficiency and cost reduction.\n",
      "- Electrical And Electronics Systems Engineering : Focuses on electrical and electronic systems design, maintenance, analysis, and project coordination.\n",
      "- Community Engagement And Cultural Development : Focuses on creating and managing social, educational, and cultural community programs.\n",
      "- Tourism And Hospitality Guest Services : Focuses on enhancing guest experiences and managing tourism-related services and operations.\n",
      "- Visual Arts And Studio Production : Creating, managing, and coordinating visual arts and studio production activities.\n",
      "- Cultural Information And Records Management : Managing and preserving cultural records, ensuring accessibility, compliance, and documentation standards.\n",
      "- Design Research And Innovation Management : Focuses on user research, product development, and managing innovation projects in design.\n",
      "- Fiber And Paper Manufacturing Operations : Covers production, quality control, and sustainable practices in fiber and paper industries.\n",
      "- Food Production Quality And Safety Management : Ensuring quality and safety standards in food production and development processes.\n",
      "If none of them or not interested by a job, just say it\n",
      "User: The last one‚Ä¶ about live events and technical stuff.\n",
      "Assistant: Are you open to relocate and what is your work type preference: remote, onsite or any ?\n",
      "User: No, I stay in Salvador. Onsite‚Ä¶ for shows.\n",
      "Assistant: What is your highest completed education level? And how many years of experience do you have?\n",
      "User: I did tecn√≥logo‚Ä¶ and 2 years in shows. Small ones.\n",
      "Assistant: Which languages do you speak and your proficiency level (B√°sico, Intermedi√°rio, Avan√ßado) ?\n",
      "User: Portugu√™s‚Ä¶ intermedi√°rio. Only that.\n",
      "Assistant: In a single sentence, which domains you are interested in to find a job from follwing list:\n",
      "- Financial Operations And Compliance Management : Managing financial activities, ensuring regulatory compliance, and supporting risk management in finance.\n",
      "- Legal Research And Procedural Support : Conducting legal research, drafting documents, and supporting procedural compliance and case management.\n",
      "- Insurance Operations And Compliance Management : Managing insurance processes, ensuring compliance, assessing risks, and handling client services.\n",
      "- Maritime And Port Operations Management : Managing and coordinating vessel, terminal, and cargo operations in maritime and port environments.\n",
      "- Industrial Production And Live Event Technical Operations : Encompasses equipment design, maintenance, and coordination in industrial and live event settings.\n",
      "- Health Safety And Environmental Compliance : Ensuring workplace safety, regulatory compliance, and environmental health in industrial settings.\n",
      "- Procurement And Supplier Management : Managing supplier relationships and optimizing procurement processes for efficiency and cost reduction.\n",
      "- Electrical And Electronics Systems Engineering : Focuses on electrical and electronic systems design, maintenance, analysis, and project coordination.\n",
      "- Community Engagement And Cultural Development : Focuses on creating and managing social, educational, and cultural community programs.\n",
      "- Tourism And Hospitality Guest Services : Focuses on enhancing guest experiences and managing tourism-related services and operations.\n",
      "- Visual Arts And Studio Production : Creating, managing, and coordinating visual arts and studio production activities.\n",
      "- Cultural Information And Records Management : Managing and preserving cultural records, ensuring accessibility, compliance, and documentation standards.\n",
      "- Design Research And Innovation Management : Focuses on user research, product development, and managing innovation projects in design.\n",
      "- Fiber And Paper Manufacturing Operations : Covers production, quality control, and sustainable practices in fiber and paper industries.\n",
      "- Food Production Quality And Safety Management : Ensuring quality and safety standards in food production and development processes.\n",
      "If none of them or not interested by a job, just say it\n",
      "User: The last one‚Ä¶ about live events and technical stuff.\n",
      "Assistant: Are you open to relocate and what is your work type preference: remote, onsite or any ?\n",
      "User: No, I stay in Salvador. Onsite‚Ä¶ for shows.\n",
      "Assistant: What is your highest completed education level? And how many years of experience do you have?\n",
      "User: I did tecn√≥logo‚Ä¶ and 2 years in shows. Small ones.\n",
      "Assistant: Which languages do you speak and your proficiency level (B√°sico, Intermedi√°rio, Avan√ßado) ?\n",
      "User: Portugu√™s‚Ä¶ intermedi√°rio. Only that.\n",
      "\n",
      "\n",
      "==================================================\n",
      "{'name': '', 'age': 18, 'location': 'Salvador', 'recommendation_type': 'jobs_trainings', 'open_to_relocate_for_work': False, 'work_type_preference': 'onsite', 'target_domains': ['Industrial Production And Live Event Technical Operations'], 'education_level': 'Tecn√≥logo', 'years_of_experience': 2, 'skills_domains': [], 'skills': {}, 'languages': ['Portuguese'], 'goals': 'Work in shows, specifically in live events and technical operations', 'hard_filtered_jobs_ids': [], 'proposed_job_ids': []}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    persona_id = 'persona_017'\n",
    "    \n",
    "    filename = f\"job_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "    personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "    personas_data = read_json(personas_save_path)\n",
    "\n",
    "    persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "    if persona_data_dict['recommendation_type'] != 'jobs_trainings':\n",
    "        print(\"recommendation_type not jobs_trainings\")\n",
    "        personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "    else:\n",
    "        conversation = initial_interviews[persona_id]['interview']\n",
    "        conversation += job_extension_interviews[persona_id]['interview']\n",
    "        \n",
    "        domains_str = \"\"\n",
    "        for domain in jobs_map:\n",
    "            domains_str += f\"- {domain} : {jobs_map[domain]['description']}\" + \"\\n\"\n",
    "        \n",
    "        result = extract_job_persona_info(\n",
    "            domains_str,\n",
    "            conversation,\n",
    "            persona_data_dict['goals'],\n",
    "            model=\"mistral-medium-latest\",\n",
    "            print_prompt=True\n",
    "        )\n",
    "    \n",
    "        domain_issue = False\n",
    "        for domain in result.target_domains:\n",
    "            if domain not in jobs_map:\n",
    "                domain_issue = True\n",
    "                print(f\"{persona_id} : {result.target_domains} not in domains list\")\n",
    "    \n",
    "        persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "        persona_data_dict['open_to_relocate_for_work'] = result.open_to_relocate_for_work\n",
    "        persona_data_dict['work_type_preference'] = result.work_type_preference\n",
    "        persona_data_dict['education_level'] = result.education_level\n",
    "        persona_data_dict['years_of_experience'] = result.years_of_experience\n",
    "        persona_data_dict['languages'] = result.languages\n",
    "        persona_data_dict['goals'] = result.goals\n",
    "        persona_data_dict['target_domains'] = result.target_domains\n",
    "\n",
    "        if persona_data_dict['education_level'] == 'T√©cn√≥logo':\n",
    "            persona_data_dict['education_level'] = 'Tecn√≥logo'\n",
    "\n",
    "        print(persona_data_dict)\n",
    "        \n",
    "        personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "    \n",
    "save_json(personas_save_path, personas_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2d6379-10af-491f-9918-ad61974aeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "You are a location harmonization agent. Your task is to process a single persona's location string provided as \"Salvador\".\n",
      "\n",
      "Follow these rules:\n",
      "1.Trim and clean the input string.\n",
      "2.If the input contains a city followed by a country (e.g., \"Fortaleza, Brazil\"), extract and normalize the city name only.\n",
      "3.VERY IMPORTANT : Normalize known aliases:\n",
      "- \"Rio\" ‚Üí \"Rio de Janeiro\"\n",
      "- \"Recife, Brazil\" ‚Üí \"Recife\"\n",
      "- \"Fortaleza, Brazil\" ‚Üí \"Fortaleza\"\n",
      "4.If the input is empty, ambiguous (e.g., \":\", \"Brazil\"), or not a valid city, return \"Unknown\".\n",
      "5.Return a Python object of class LocationNameRefator with:\n",
      "- renaming_needed = True if the name was changed or cleaned.\n",
      "- renaming_needed = False if the input was already clean.\n",
      "- name = \"<standardized city name>\" or \"Unknown\" if invalid.\n",
      "- rationale = justification of your decision\n",
      "\n",
      "\n",
      "==================================================\n",
      "{'name': '', 'age': 18, 'location': 'Salvador', 'recommendation_type': 'jobs_trainings', 'open_to_relocate_for_work': False, 'work_type_preference': 'onsite', 'target_domains': ['Industrial Production And Live Event Technical Operations'], 'education_level': 'Tecn√≥logo', 'years_of_experience': 2, 'skills_domains': [], 'skills': {}, 'languages': ['Portuguese'], 'goals': 'Work in shows, specifically in live events and technical operations', 'hard_filtered_jobs_ids': [], 'proposed_job_ids': []}\n"
     ]
    }
   ],
   "source": [
    "# check and correct location information\n",
    "if True:\n",
    "    persona_id = 'persona_017'\n",
    "    \n",
    "    filename = f\"job_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "    personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "    initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "    filename = f\"final_personas_info_{personas_info_data_version}.json\"\n",
    "    personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "    personas_data = read_json(personas_save_path)\n",
    "\n",
    "    persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "    result = location_name_refactoring(\n",
    "        persona_data_dict['location'],\n",
    "        model=\"mistral-medium-latest\",\n",
    "        print_prompt=True\n",
    "    )\n",
    "    \n",
    "    if result.renaming_needed is True:\n",
    "        print(f\"{persona_data_dict['location']} => {result.name}\")\n",
    "        persona_data_dict['location'] = result.name\n",
    "\n",
    "    personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "    print(persona_data_dict)\n",
    "        \n",
    "    save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fdc76-fe05-4795-a3e6-44bd8ecec3f4",
   "metadata": {},
   "source": [
    "# Persona info statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daac4b36-e1e1-4f63-9023-b032ff814cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "persona_infos = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be33d968-96ef-4033-bcc0-2f8a797a720d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 13: 2 occurrences\n",
      "Age 14: 1 occurrences\n",
      "Age 15: 5 occurrences\n",
      "Age 16: 10 occurrences\n",
      "Age 17: 3 occurrences\n",
      "Age 18: 12 occurrences\n",
      "Age 19: 7 occurrences\n",
      "Age 20: 3 occurrences\n",
      "Age 21: 7 occurrences\n",
      "Age 22: 7 occurrences\n",
      "Age 23: 4 occurrences\n",
      "Age 24: 4 occurrences\n",
      "Age 25: 7 occurrences\n",
      "Age 26: 8 occurrences\n",
      "Age 27: 9 occurrences\n",
      "Age 28: 7 occurrences\n",
      "Age 29: 1 occurrences\n",
      "Age 30: 2 occurrences\n",
      "\n",
      "Recommendation Type Counts:\n",
      "jobs_trainings: 55\n",
      "trainings_only: 26\n",
      "awareness: 19\n",
      "\n",
      "Total count: 100\n",
      "\n",
      "Location Counts:\n",
      "S√£o Paulo: 12\n",
      "Recife: 12\n",
      "Rio de Janeiro: 12\n",
      "Belo Horizonte: 12\n",
      "Bras√≠lia: 11\n",
      "Curitiba: 10\n",
      "Salvador: 9\n",
      "Porto Alegre: 8\n",
      ": 6\n",
      "Fortaleza: 5\n",
      "Brazil: 2\n",
      "Unknown: 1\n",
      "\n",
      "Total count: 100\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract ages from the JSON data\n",
    "ages = []\n",
    "for persona in persona_infos.values():\n",
    "    data = json.loads(persona)\n",
    "    if data['age'] > 0:  # Filter out invalid age (0)\n",
    "        ages.append(data['age'])\n",
    "age_counts = Counter(ages)\n",
    "# Print in ascending order of age\n",
    "for age in sorted(age_counts.keys()):\n",
    "    print(f\"Age {age}: {age_counts[age]} occurrences\")\n",
    "\n",
    "# Extract infos from the dictionary\n",
    "recommendation_types = []\n",
    "locations = []\n",
    "for persona in persona_infos.values():\n",
    "    data = json.loads(persona)\n",
    "    recommendation_types.append(data['recommendation_type'])\n",
    "    locations.append(data['location'])\n",
    "\n",
    "type_counts = Counter(recommendation_types)\n",
    "print(\"\\nRecommendation Type Counts:\")\n",
    "for type_name, count in type_counts.most_common():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "print(f\"\\nTotal count: {sum(type_counts.values())}\")\n",
    "\n",
    "type_counts = Counter(locations)\n",
    "print(\"\\nLocation Counts:\")\n",
    "for type_name, count in type_counts.most_common():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "print(f\"\\nTotal count: {sum(type_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873b4a4-1490-4871-ae58-9d13060138cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19224f59-4df5-460d-a775-5e335fb97d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Testing Persona information Extraction Agent...\n",
      "Reading a sample job file...\n",
      "\n",
      "name='Camila' age=22 location='Fortaleza' recommendation_type='trainings_only' open_to_relocate_for_work=False work_type_preference='onsite' target_domains=['UNKNOWN'] education_level='T√©cnico' years_of_experience=0 skills_domains=['UNKNOWN'] skills={} languages={'Portuguese', 'English'} goals='Learn all basics of live event production: lights, sound, stage setup.'\n"
     ]
    }
   ],
   "source": [
    "# Correct Persona Info Extraction\n",
    "print(\"üíº Testing Persona information Extraction Agent...\")\n",
    "print(\"Reading a sample job file...\\n\")\n",
    "\n",
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)\n",
    "\n",
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Get first job file\n",
    "persona_id = \"persona_015\"\n",
    "\n",
    "if persona_id in interviews:\n",
    "    conversation = interviews[persona_id]\n",
    "\n",
    "    # Extract\n",
    "    persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "    personas_data[persona_id] = persona_info.model_dump_json()\n",
    "    save_json(personas_save_path, personas_data)\n",
    "\n",
    "    print(persona_info)\n",
    "    #persona_infos[persona_id] = persona_info.model_dump_json()\n",
    "    \n",
    "    # persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025f470-5693-451a-a5d3-08502b9162fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
