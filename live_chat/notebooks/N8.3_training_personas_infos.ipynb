{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aec46e5-52bf-4fcd-b868-63fbab04bf54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting strands-agents[mistral]\n",
      "  Downloading strands_agents-1.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from strands-agents[mistral])\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mcp<2.0.0,>=1.11.0 (from strands-agents[mistral])\n",
      "  Downloading mcp-1.19.0-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Collecting mistralai>=1.8.2 (from strands-agents[mistral])\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Collecting opentelemetry-instrumentation==0.59b0 (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n",
      "Downloading strands_agents-1.13.0-py3-none-any.whl (223 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.19.0-py3-none-any.whl (170 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl (9.3 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: python-multipart, python-dotenv, invoke, httpx-sse, httpcore, eval-type-backport, docstring-parser, opentelemetry-api, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, httpx, opentelemetry-sdk, opentelemetry-instrumentation, mistralai, mcp, opentelemetry-instrumentation-threading, strands-agents\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18/18\u001b[0m [strands-agents]m [strands-agents]dk]onventions]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 invoke-2.2.1 mcp-1.19.0 mistralai-1.9.11 opentelemetry-api-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-threading-0.59b0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-settings-2.11.0 python-dotenv-1.2.1 python-multipart-0.0.20 sse-starlette-3.0.2 strands-agents-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76fec45b-aa58-44fb-827c-0175956f2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "\tchat_with_persona,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract,\n",
    "    compute_stat_for_multi_items\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo, PersonaSkills\n",
    "from src.models.activity_domain_info import ActivityDomainInfo, ListOfActivityDomains\n",
    "from src.models.skill_domain_info import SkillDomainInfo, ListSkillsDomains\n",
    "from src.prompts.persona_extraction_prompt import (\n",
    "    PERSONA_SKILLS_EXTRACTION_PROMPT\n",
    ")\n",
    "\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ùå No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"‚úÖ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e41af00-3ed7-4d1f-8bd0-b16446186248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_INTERVIEWS_DIR = Path('../data_interviews')\n",
    "DATA_ACTIVITIES_DOMAINS_DIR = Path('../data_activities_domains')\n",
    "DATA_SKILLS_DOMAINS_DIR = Path('../data_skills_domains')\n",
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7b5960a-d12b-4af8-b533-ba27fed5f106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "interview_data_version version: v8\n",
      "activity_domains_version version: v4\n",
      "personas_info_data_version version: v14\n",
      "skill_domains_version version: v3\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "interview_data_version = config[\"interview_data_version\"]\n",
    "print(f\"interview_data_version version: {interview_data_version}\")\n",
    "\n",
    "activity_domains_version = config[\"activity_domains_version\"]\n",
    "print(f\"activity_domains_version version: {activity_domains_version}\")\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "skill_domains_version = config[\"skill_domains_version\"]\n",
    "print(f\"skill_domains_version version: {skill_domains_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06806ca7-5fe3-4fb0-a851-57938ac909c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jobs domains map data\n",
    "filename = f\"map_clusters_jobs_{job_data_version}.json\"\n",
    "save_path = DATA_JOBS_DIR / filename\n",
    "jobs_map = read_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb17430b-2038-4b5f-9397-fe62d8395a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 12 skills domains\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load skills domains map data\n",
    "filename = f\"final_map_clusters_trainings_{training_data_version}.json\"\n",
    "save_path = DATA_TRAININGS_DIR / filename\n",
    "trainings_map = read_json(save_path)\n",
    "trainings_map_lower = {key.lower(): value for key, value in trainings_map.items()}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(trainings_map)} skills domains\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f7e10af-6bfa-4ce1-b458-7e76a7c44277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interviews\n",
    "filename = f\"training_skills_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "initial_interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4f22bba-8ccc-49f8-b6b5-8675a90e87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 personas\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Personas data\n",
    "filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "\n",
    "initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in initial_personas_data.items()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(personas)} personas\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b72c5-8749-46e7-901c-7ed9058fcd7f",
   "metadata": {},
   "source": [
    "# Extract informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af10406c-e2f9-4f05-a4d5-e84fcebf8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training_persona_info(\n",
    "    skills_str: str,\n",
    "    conversation: List[str],\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> PersonaSkills:\n",
    "    text = '\\n'.join(conversation)\n",
    "    #print(text)\n",
    "\n",
    "    prompt = PERSONA_SKILLS_EXTRACTION_PROMPT.format(\n",
    "        skills_list=skills_str,\n",
    "        conversation=text\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(prompt)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=PersonaSkills, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e945ced-292e-4a45-9173-8ee331b27eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_LOOPS = 1\n",
    "cache_period = 5\n",
    "\n",
    "# Prepare personas info\n",
    "filename = f\"last_final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "# personas_data = {}\n",
    "\n",
    "new_items_processed = 0\n",
    "\n",
    "print(\"START CLASSIFICATION LOOP\")\n",
    "for i in range(MAX_LOOPS):\n",
    "    print(\"ITERATE CLASSIFICATION LOOP\")\n",
    "\n",
    "    new_personas_processed = 0\n",
    "\n",
    "    for persona_id in tqdm(initial_personas_data):\n",
    "        #print(persona_id)\n",
    "        \n",
    "        if persona_id in personas_data:\n",
    "            initial_persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "            if initial_persona_data_dict['recommendation_type'] != 'trainings_only':\n",
    "                personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "            continue\n",
    "\n",
    "        if personas[persona_id].recommendation_type != 'trainings_only':\n",
    "            personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "            continue\n",
    "\n",
    "        if persona_id not in initial_interviews:\n",
    "            personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "            continue\n",
    "\n",
    "        new_personas_processed += 1\n",
    "\n",
    "        conversation = initial_interviews[persona_id]['interview']\n",
    "\n",
    "        personas_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "        skill_domains = personas_data_dict['skills_domains']\n",
    "\n",
    "        if len(skill_domains) < 1:\n",
    "            print(f\"Persona {persona_id} has no skill domain\")\n",
    "            persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "            print(persona_data_dict)\n",
    "            persona_data_dict['skills'] = {}\n",
    "        else:\n",
    "            skills_str = \"\"\n",
    "            for domain in skill_domains:\n",
    "                if domain.lower() not in trainings_map_lower:\n",
    "                #if domain not in trainings_map:\n",
    "                    print(f\"{persona_id} - {domain} not in trainings_map\")\n",
    "                    continue\n",
    "                for skill in trainings_map_lower[domain.lower()]:\n",
    "                    skills_str += f\"- {domain} : {skill}\" + \"\\n\"\n",
    "    \n",
    "            print_prompt = False\n",
    "            if new_personas_processed == 1:\n",
    "                print_prompt = True\n",
    "\n",
    "            #if persona_id == 'persona_067':\n",
    "            #    print_prompt = True\n",
    "    \n",
    "            result = extract_training_persona_info(\n",
    "                skills_str,\n",
    "                conversation,\n",
    "                model=\"mistral-medium-latest\",\n",
    "                print_prompt=print_prompt\n",
    "            )\n",
    "    \n",
    "            # if len(result.target_domains) == 0:\n",
    "            #     print(f\"Activity domains empty : for {persona_id}\")\n",
    "            #     continue\n",
    "    \n",
    "            # domain_issue = False\n",
    "            # for domain in result.target_domains:\n",
    "            #     if domain not in jobs_map:\n",
    "            #         domain_issue = True\n",
    "            #         print(f\"{persona_id} : {result.target_domains} not in domains list\")\n",
    "    \n",
    "            # if domain_issue is True:\n",
    "            #     continue\n",
    "    \n",
    "            persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "\n",
    "            if result.interested_by_training is False:\n",
    "                persona_data_dict['recommendation_type'] = 'awareness'\n",
    "                print(persona_id)\n",
    "                print('recommendation_type = awareness')\n",
    "            else:            \n",
    "                persona_data_dict['skills'] = result.skills\n",
    "        \n",
    "        personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "\n",
    "        # Save every 5 personas\n",
    "        if new_personas_processed % 5 == 0:\n",
    "            save_json(personas_save_path, personas_data)\n",
    "\n",
    "        # Show cost update every 20 personas\n",
    "        # if new_personas_processed > 0 and new_personas_processed % 20 == 0:\n",
    "        #     print(f\"\\nüí∞ Cost update after {new_personas_processed} new personas:\")\n",
    "        #     print_cost_summary()\n",
    "        #     print()\n",
    "\n",
    "        # if new_personas_processed > 0:break\n",
    "\n",
    "save_json(personas_save_path, personas_data)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "# personas = {\n",
    "#     pid: PersonaInfo.model_validate_json(data)\n",
    "#     for pid, data in persona_infos.items()\n",
    "# }\n",
    "\n",
    "# print(f\"\\n‚úÖ Interviewed {len(personas)} personas total ({new_personas_processed} new)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b4200-6b68-48aa-9ea7-9eb73be72ab6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08ce89e9-64f8-4a6a-83cd-b0741d85df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    persona_id = \"persona_059\"\n",
    "\n",
    "    conversation = initial_interviews[persona_id]['interview']\n",
    "\n",
    "    personas_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "    skill_domains = personas_data_dict['skills_domains']\n",
    "\n",
    "    skills_str = \"\"\n",
    "    for domain in skill_domains:\n",
    "        for skill in trainings_map[domain]:\n",
    "            skills_str += f\"- {domain} : {skill}\" + \"\\n\"\n",
    "\n",
    "    result = extract_training_persona_info(\n",
    "        skills_str,\n",
    "        conversation,\n",
    "        model=\"mistral-medium-latest\",\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    #persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "    #persona_data_dict['open_to_relocate_for_work'] = result.open_to_relocate_for_work\n",
    "    #persona_data_dict['work_type_preference'] = result.work_type_preference\n",
    "    #persona_data_dict['education_level'] = result.education_level\n",
    "    #persona_data_dict['years_of_experience'] = result.years_of_experience\n",
    "    #persona_data_dict['languages'] = result.languages\n",
    "    #persona_data_dict['goals'] = result.goals\n",
    "    #persona_data_dict['target_domains'] = result.target_domains\n",
    "\n",
    "    #print(persona_data_dict)\n",
    "\n",
    "    # print(persona_info)\n",
    "\n",
    "    # if(persona_info.recommendation_type != \"awareness\"):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f39c-c11e-467e-98bc-95a1485637c5",
   "metadata": {},
   "source": [
    "# Redo Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64e45f70-3478-499e-818c-494ffd5d35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "You are an advanced AI assistant specializing in skill extraction from text. Your task is to analyze a conversation and a corresponding list of skills to identify which skills a persona is interested in and their current proficiency level.\n",
      "\n",
      "You will be given the following inputs:\n",
      "- a list of required skills for a job application : [LIST OF AVAILABLE SKILLS]\n",
      "- an interview conversation with a persona. : [CONVERSATION]\n",
      "\n",
      "[LIST OF AVAILABLE SKILLS]:\n",
      "\n",
      "[END OF LIST OF AVAILABLE SKILLS]\n",
      "\n",
      "[CONVERSATION]\n",
      "Assistant: From following list of skills, for which you are interested in and what is your current proficiency level (None, Basic, Intermediate, Advanced):\n",
      "- Financial Risk Management And Compliance : Financial Software Proficiency\n",
      "- Financial Risk Management And Compliance : Cost Analysis And Financial Evaluation\n",
      "- Financial Risk Management And Compliance : Financial Data Analysis\n",
      "- Financial Risk Management And Compliance : Financial Compliance Reporting\n",
      "- Financial Risk Management And Compliance : Corporate Financial Reporting\n",
      "- Financial Risk Management And Compliance : Regulatory Compliance Management\n",
      "- Financial Risk Management And Compliance : Actuarial Science\n",
      "- Financial Risk Management And Compliance : Insurance Claims Processing\n",
      "- Financial Risk Management And Compliance : Insurance Legal Agreements Interpretation\n",
      "- Financial Risk Management And Compliance : Business Risk Management\n",
      "- Financial Risk Management And Compliance : Intermediate Client Interaction And Problem Resolution\n",
      "- Financial Risk Management And Compliance : Insurance Pricing Analytics\n",
      "- Financial Risk Management And Compliance : Fraud Detection And Prevention\n",
      "- Financial Risk Management And Compliance : Risk Assessment\n",
      "- Financial Risk Management And Compliance : Insurance Risk Assessment And Decision Making\n",
      "- Financial Risk Management And Compliance : Insurance Sales And Marketing\n",
      "- Financial Risk Management And Compliance : Strategic Planning And Analysis\n",
      "- Financial Risk Management And Compliance : Tax Regulation And Compliance\n",
      "- Financial Risk Management And Compliance : Financial Auditing\n",
      "- Financial Risk Management And Compliance : Customer Information Systems Management\n",
      "- Financial Risk Management And Compliance : Regulatory Supervision\n",
      "- Financial Risk Management And Compliance : Financial Data Security\n",
      "- Financial Risk Management And Compliance : Document Management\n",
      "- Financial Risk Management And Compliance : Financial Crime Prevention And Detection\n",
      "- Financial Risk Management And Compliance : Cross-Functional Coordination And Communication\n",
      "- Financial Risk Management And Compliance : Process Optimization In Financial Services\n",
      "- Financial Risk Management And Compliance : Organizational Policy Implementation\n",
      "- Financial Risk Management And Compliance : Risk Documentation Management\n",
      "- Financial Risk Management And Compliance : Financial Planning And Budgeting\n",
      "If none of them or not interested by a training, just say it\n",
      "User: The only one that fits what I‚Äôm looking for is **Organizational Policy Implementation**‚Äîmy level is basic, since I‚Äôve just started. The rest feels too advanced or not really what I need for admin work in banking or insurance.\n",
      "[END OF CONVERSATION]\n",
      "\n",
      "From the conversation, extract the skill names for which the persona is interested and their current proficiency level, limited to: None, Basic, Intermediate, or Advanced.\n",
      "\n",
      "# Guidelines (IMPORTANT !) :\n",
      "1. Analyse user feedback about user motivation to take trainings.\n",
      "2. If the user expresses any hesitation, doubt, or explicitly says they do not need or want training (e.g., ‚ÄúI don‚Äôt need training yet‚Äù), you MUST set interested_by_training=False. Otherwise, set True.\n",
      "3. Extract only skills that persona express interest in and that are present in available skills list.\n",
      "4. Use exact same skill name that provided in the available skills list. Do not change any letter or caps. Do not add formating characters.\n",
      "5. Return only the skills and proficency level. Don't add any explanation or additional text (IMPORTANT)\n",
      "\n",
      "\n",
      "==================================================\n",
      "skills={'Financial Risk Management And Compliance : Organizational Policy Implementation': 'Basic'} interested_by_training=True rationale='The user explicitly stated interest in training for \"Organizational Policy Implementation\" and mentioned their proficiency level as basic.'\n",
      "{'name': '', 'age': 17, 'location': 'Recife', 'recommendation_type': 'trainings_only', 'open_to_relocate_for_work': False, 'work_type_preference': 'onsite', 'target_domains': ['Insurance Operations And Compliance Management'], 'education_level': 'T√©cnico', 'years_of_experience': 1, 'skills_domains': ['Financial Risk Management And Compliance'], 'skills': {'Financial Risk Management And Compliance : Organizational Policy Implementation': 'Basic'}, 'languages': ['Portuguese'], 'goals': 'Exploring career paths in Policy Implementation for banking and insurance. Currently working in admin at a local insurance place and interested in improving skills related to insurance policies.', 'hard_filtered_jobs_ids': ['j90'], 'proposed_job_ids': ['j90']}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    persona_id = 'persona_067'\n",
    "\n",
    "    filename = f\"last_final_personas_info_{personas_info_data_version}.json\"\n",
    "    personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "    personas_data = read_json(personas_save_path)\n",
    "\n",
    "    if personas[persona_id].recommendation_type != 'trainings_only':\n",
    "        print(\"Not classified as training only\")\n",
    "        personas_data[persona_id] = initial_personas_data[persona_id]\n",
    "        save_json(personas_save_path, personas_data)\n",
    "        print(personas_data[persona_id])\n",
    "    elif persona_id not in initial_interviews:\n",
    "        print(\"ISSUE\")\n",
    "    else:\n",
    "        conversation = initial_interviews[persona_id]['interview']\n",
    "\n",
    "        personas_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "        skill_domains = personas_data_dict['skills_domains']\n",
    "\n",
    "        if len(skill_domains) < 1:\n",
    "            print(\"Persona has no skill domain\")\n",
    "            persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "            print(persona_data_dict)\n",
    "            persona_data_dict['skills'] = {}\n",
    "        else:\n",
    "            skills_str = \"\"\n",
    "            for domain in skill_domains:\n",
    "                if domain.lower() not in trainings_map_lower:\n",
    "                #for skill in trainings_map[domain]:\n",
    "                    for skill in trainings_map_lower[domain.lower()]:\n",
    "                        skills_str += f\"- {domain} : {skill}\" + \"\\n\"\n",
    "\n",
    "            result = extract_training_persona_info(\n",
    "                skills_str,\n",
    "                conversation,\n",
    "                model=\"mistral-medium-latest\",\n",
    "                print_prompt=True\n",
    "            )\n",
    "\n",
    "            print(result)\n",
    "\n",
    "            persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "            if result.interested_by_training is False:\n",
    "                persona_data_dict['recommendation_type'] = 'awareness'\n",
    "            else:            \n",
    "                persona_data_dict['skills'] = result.skills\n",
    "\n",
    "        personas_data[persona_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "        print(persona_data_dict)\n",
    "        \n",
    "        save_json(personas_save_path, personas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fdc76-fe05-4795-a3e6-44bd8ecec3f4",
   "metadata": {},
   "source": [
    "# Persona skills quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daac4b36-e1e1-4f63-9023-b032ff814cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"last_final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "persona_infos = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be33d968-96ef-4033-bcc0-2f8a797a720d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 84990.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for persona_id in tqdm(persona_infos):\n",
    "    if personas[persona_id].recommendation_type != 'trainings_only':\n",
    "        continue\n",
    "\n",
    "    persona_data_dict = json.loads(persona_infos[persona_id])\n",
    "\n",
    "    if len(persona_data_dict['skills']) == 0:\n",
    "        continue\n",
    "\n",
    "    for domain_skill in persona_data_dict['skills']:\n",
    "        level = persona_data_dict['skills'][domain_skill]\n",
    "        parts = domain_skill.split(\" : \")\n",
    "        if len(parts) < 2:\n",
    "            print(f\"ERROR {persona_id} : format error '{parts}'\")\n",
    "            continue\n",
    "\n",
    "        domain = parts[0]\n",
    "        skill = parts[1]\n",
    "\n",
    "        if domain not in trainings_map:\n",
    "            print(f\"ERROR {persona_id} : domain not knwon '{domain}'\")\n",
    "            continue\n",
    "\n",
    "        if skill not in trainings_map[domain]:\n",
    "            print(f\"ERROR {persona_id} : skill not knwon '{skill}'\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873b4a4-1490-4871-ae58-9d13060138cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19224f59-4df5-460d-a775-5e335fb97d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Testing Persona information Extraction Agent...\n",
      "Reading a sample job file...\n",
      "\n",
      "name='Camila' age=22 location='Fortaleza' recommendation_type='trainings_only' open_to_relocate_for_work=False work_type_preference='onsite' target_domains=['UNKNOWN'] education_level='T√©cnico' years_of_experience=0 skills_domains=['UNKNOWN'] skills={} languages={'Portuguese', 'English'} goals='Learn all basics of live event production: lights, sound, stage setup.'\n"
     ]
    }
   ],
   "source": [
    "# Correct Persona Info Extraction\n",
    "print(\"üíº Testing Persona information Extraction Agent...\")\n",
    "print(\"Reading a sample job file...\\n\")\n",
    "\n",
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)\n",
    "\n",
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Get first job file\n",
    "persona_id = \"persona_015\"\n",
    "\n",
    "if persona_id in interviews:\n",
    "    conversation = interviews[persona_id]\n",
    "\n",
    "    # Extract\n",
    "    persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "    personas_data[persona_id] = persona_info.model_dump_json()\n",
    "    save_json(personas_save_path, personas_data)\n",
    "\n",
    "    print(persona_info)\n",
    "    #persona_infos[persona_id] = persona_info.model_dump_json()\n",
    "    \n",
    "    # persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025f470-5693-451a-a5d3-08502b9162fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
