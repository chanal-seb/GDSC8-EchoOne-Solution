{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516a2296-3eb9-4192-be81-e40e80184284",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting strands-agents[mistral]\n",
      "  Downloading strands_agents-1.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from strands-agents[mistral])\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mcp<2.0.0,>=1.11.0 (from strands-agents[mistral])\n",
      "  Downloading mcp-1.19.0-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Collecting mistralai>=1.8.2 (from strands-agents[mistral])\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Collecting opentelemetry-instrumentation==0.59b0 (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n",
      "Downloading strands_agents-1.13.0-py3-none-any.whl (223 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.19.0-py3-none-any.whl (170 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl (9.3 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: python-multipart, python-dotenv, invoke, httpx-sse, httpcore, eval-type-backport, docstring-parser, opentelemetry-api, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, httpx, opentelemetry-sdk, opentelemetry-instrumentation, mistralai, mcp, opentelemetry-instrumentation-threading, strands-agents\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [strands-agents]m [strands-agents]dk]onventions]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 invoke-2.2.1 mcp-1.19.0 mistralai-1.9.11 opentelemetry-api-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-threading-0.59b0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-settings-2.11.0 python-dotenv-1.1.1 python-multipart-0.0.20 sse-starlette-3.0.2 strands-agents-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ec557d1-6657-41da-bc56-e7e93520d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "from collections import Counter\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "    chat_with_persona,\n",
    "    validate_submission_format,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo\n",
    "from src.models.job_info import JobInfo\n",
    "from src.models.training_info import TrainingInfo\n",
    "from src.models.generic_models import (\n",
    "    BooleanModel,\n",
    "    BooleanModelWithRationale,\n",
    "    ListOfIds,\n",
    "    ListOfStrs\n",
    ")\n",
    "\n",
    "from src.prompts.find_training_matches_prompt import (\n",
    "    FIND_TRAINING_MATCHES_PROMPT,\n",
    "    FIND_TRAINING_MATCHES_PROMPT_BY_NAME,\n",
    "    CHECK_PERSONA_TRAINING_MATCH,\n",
    "    FIND_TRAINING_MATCHES_FOR_JOB_PROMPT\n",
    ")\n",
    "\n",
    "from src.prompts.find_job_matches_prompt import (\n",
    "    FIND_JOB_MATCHES_PROMPT\n",
    ")\n",
    "\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"❌ No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"✅ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78128367-0d4c-45d6-a210-bad41552425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix = 'job_filtered_personas_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6626b2a-6868-4572-9f69-b17dc9a2decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')\n",
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_SKILLS_DOMAINS_DIR = Path('../data_skills_domains')\n",
    "DATA_MATCH_JOBS_TRAININGS_DIR = Path('../data_match_jobs_trainings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47617cdc-361c-402b-9ed6-db72f326c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas_info_data_version version: v14\n",
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "skill_domains_version version: v3\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "skill_domains_version = config[\"skill_domains_version\"]\n",
    "print(f\"skill_domains_version version: {skill_domains_version}\")\n",
    "\n",
    "match_skills_domains_trainings_data_version = f\"{skill_domains_version}_{training_data_version}\"\n",
    "match_jobs_trainings_data_version = f\"{job_data_version}_{training_data_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c562606-1140-4876-b26c-b3e485eb4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_DIR = Path('../submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9eb02245-6ac1-4587-83c9-c8a4ec1b8d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 200 jobs\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Jobs data\n",
    "filename = f\"final_jobs_{job_data_version}.json\"\n",
    "#filename = f\"final_jobs_{job_data_version}.json\"\n",
    "jobs_save_path = DATA_JOBS_DIR / filename\n",
    "\n",
    "jobs_data = read_json(jobs_save_path)\n",
    "\n",
    "# Convert to JobInfo objects\n",
    "jobs_info = {\n",
    "    job_id: JobInfo.model_validate_json(data)\n",
    "    for job_id, data in jobs_data.items()\n",
    "}\n",
    "\n",
    "print(f\"✅ Loaded {len(jobs_info)} jobs\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d6abdf6-88d0-4d27-ae70-c91fda0457fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 497 trainings\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Trainings data\n",
    "filename = f\"extended_trainings_{training_data_version}.json\"\n",
    "trainings_save_path = DATA_TRAININGS_DIR / filename\n",
    "\n",
    "trainings_data = read_json(trainings_save_path)\n",
    "\n",
    "# Convert to TrainingInfo objects\n",
    "trainings_info = {\n",
    "    training_id: TrainingInfo.model_validate_json(data)\n",
    "    for training_id, data in trainings_data.items()\n",
    "}\n",
    "\n",
    "print(f\"✅ Loaded {len(trainings_info)} trainings\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa80e271-2398-41a6-9fa6-e38cf0a703b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 99 personas\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Personas data\n",
    "filename = f\"final_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "\n",
    "initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in initial_personas_data.items()\n",
    "}\n",
    "\n",
    "print(f\"✅ Loaded {len(personas)} personas\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "356b1d7e-2bcd-4862-85fb-37261665c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jobs domains map data\n",
    "filename = f\"map_clusters_jobs_{job_data_version}.json\"\n",
    "save_path = DATA_JOBS_DIR / filename\n",
    "jobs_map = read_json(save_path)\n",
    "\n",
    "# print(jobs_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6aa71-0d0d-4755-9011-2f702e7239a6",
   "metadata": {},
   "source": [
    "# Compute Proposed Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53096907-38c6-45d5-a53e-d0bfe8d33a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_by_target_domains(persona_target_domains, jobs_map):\n",
    "    jobs_ids = []\n",
    "\n",
    "    for domain in persona_target_domains:\n",
    "        for job_id in jobs_map[domain]['job_ids']:\n",
    "            jobs_ids.append(job_id)\n",
    "    return jobs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19ba8067-e77b-4516-a1e6-b1b088b5eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_filter_jobs(persona_info, job_ids, jobs_info, verbose=False):\n",
    "    filtered_job_ids = []\n",
    "\n",
    "    for job_id in job_ids:\n",
    "        if verbose is True:\n",
    "            print(job_id)\n",
    "        job_info = jobs_info[job_id]\n",
    "\n",
    "        # if persona_info.work_type_preference == 'onsite' and job_info.work_type == 'remote':\n",
    "        #     if verbose is True:\n",
    "        #         print(f\"excluded because of work_type : {job_info.work_type} - {persona_info.work_type_preference}\")\n",
    "        #     continue\n",
    "            \n",
    "        # if persona_info.work_type_preference == 'remote' and job_info.work_type == 'onsite':\n",
    "        #     if verbose is True:\n",
    "        #         print(f\"excluded because of work_type : {job_info.work_type} - {persona_info.work_type_preference}\")\n",
    "        #     continue\n",
    "\n",
    "        if job_info.work_type == 'onsite':\n",
    "            if persona_info.open_to_relocate_for_work is False and job_info.location != persona_info.location:\n",
    "                if verbose is True:\n",
    "                    print(f\"excluded because of location : {job_info.location} - {persona_info.location}\")\n",
    "                continue\n",
    "\n",
    "        if verbose is True:\n",
    "            print(\"Location OK\")\n",
    "\n",
    "        job_education_level = job_info.get_education_level_value()\n",
    "        if job_education_level == -1:\n",
    "            print(f\"ERROR : job_education_level not recognized : {job_info.education_level_required}\")\n",
    "\n",
    "        persona_education_level = persona_info.get_education_level_value()\n",
    "        if persona_education_level == -1:\n",
    "            print(f\"ERROR : persona_education_level not recognized : {persona_info.education_level}\")\n",
    "\n",
    "        if job_education_level > persona_education_level:\n",
    "            if verbose is True:\n",
    "                print(f\"excluded because of education level : {job_education_level} - {persona_education_level}\")\n",
    "            continue\n",
    "\n",
    "        if job_info.years_of_experience_required > persona_info.years_of_experience:\n",
    "            if verbose is True:\n",
    "                print(f\"excluded because of experience : {job_info.years_of_experience_required} - {persona_info.years_of_experience}\")\n",
    "            continue\n",
    "\n",
    "        is_language_match = False\n",
    "        for job_language in job_info.required_languages:\n",
    "            for persona_language in persona_info.languages:\n",
    "                if job_language == persona_language:\n",
    "                    is_language_match = True\n",
    "        if is_language_match is False:\n",
    "            if verbose is True:\n",
    "                print(\"excluded because of language\")\n",
    "            continue\n",
    "\n",
    "        filtered_job_ids.append(job_id)\n",
    "        \n",
    "    return filtered_job_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3217d222-cb1e-441e-954e-5f1012671a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_job_matches(\n",
    "    persona_info: PersonaInfo,\n",
    "    jobs_text: str,  # Pre-built context to avoid rebuilding\n",
    "    model: str = \"mistral-medium-latest\",\n",
    "    print_prompt=False\n",
    ") -> ListOfIds:\n",
    "    \"\"\"Find suitable jobs for a persona using semantic matching\"\"\"\n",
    "\n",
    "    prompt = FIND_JOB_MATCHES_PROMPT.format(\n",
    "            candidate_profile=persona_info.goals,\n",
    "            jobs=jobs_text\n",
    "        )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    # return []\n",
    "    agent = get_agent(model_id=model, temperature=0.0)\n",
    "    response = agent.structured_output(output_model=ListOfIds, prompt=prompt)\n",
    "\n",
    "    # print(response)\n",
    "    # Track cost\n",
    "    # track_api_call(response, model)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "140c0f7a-7b62-4d33-926e-212650a1c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_agent_process_jobs(persona_id, verbose=False):\n",
    "    ###\n",
    "    # Get jobs matching persona targeted activity domains\n",
    "    ###\n",
    "    filtered_jobs_ids = get_jobs_by_target_domains(personas[persona_id].target_domains, jobs_map)\n",
    "    if verbose is True:\n",
    "        print(f\"filtered_jobs_ids : {filtered_jobs_ids}\")\n",
    "\n",
    "    ###\n",
    "    # Apply hard filters \n",
    "    ###\n",
    "    hard_filtered_jobs_ids = hard_filter_jobs(personas[persona_id], filtered_jobs_ids, jobs_info, verbose=verbose)\n",
    "    if verbose is True:\n",
    "        print(f\"hard_filtered_jobs_ids : {hard_filtered_jobs_ids}\")\n",
    "\n",
    "    if len(hard_filtered_jobs_ids) == 0:\n",
    "        # no jobs remaining\n",
    "        selected_jobs_ids = []\n",
    "        rationale = ''\n",
    "    else:    \n",
    "        ###\n",
    "        # Review job list according to persona goal\n",
    "        ###\n",
    "        jobs_text = \"\"\n",
    "        for job_id in hard_filtered_jobs_ids:\n",
    "            jobs_text += jobs_info[job_id].get_info_for_matching(job_id) + \"\\n\\n\"\n",
    "    \n",
    "        result = review_job_matches(personas[persona_id], jobs_text, print_prompt=verbose)\n",
    "        selected_jobs_ids = result.list_of_ids\n",
    "        rationale = result.rationale\n",
    "    \n",
    "    if verbose is True:\n",
    "        print(f\"selected_jobs_ids : {selected_jobs_ids}\")\n",
    "    \n",
    "    return hard_filtered_jobs_ids, selected_jobs_ids, rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f689fbf7-d546-4364-b7a3-38328ebb12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [04:18<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "cache_period = 5\n",
    "\n",
    "# Prepare personas info\n",
    "filename = f\"{output_prefix}_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "new_personas_processed = 0\n",
    "for person_id in tqdm(personas):\n",
    "    # print(person_id)\n",
    "    new_personas_processed += 1\n",
    "    persona = personas[person_id]\n",
    "    personas_data[person_id] = initial_personas_data[person_id]\n",
    "    persona_data = json.loads(initial_personas_data[person_id])\n",
    "\n",
    "    if persona.recommendation_type == \"jobs_trainings\":\n",
    "        if 'proposed_job_ids' not in persona_data:\n",
    "            length = 0\n",
    "        else:\n",
    "            length = len(persona_data['proposed_job_ids'])\n",
    "        if length == 0:\n",
    "            #print(f\"persona {person_id} without job proposed\")\n",
    "            hard_filtered_jobs_ids, selected_jobs_ids, rationale = matching_agent_process_jobs(person_id, verbose=False)\n",
    "            persona_data['hard_filtered_jobs_ids'] = hard_filtered_jobs_ids\n",
    "            persona_data['proposed_job_ids'] = selected_jobs_ids\n",
    "            personas_data[person_id] = json.dumps(persona_data)\n",
    "\n",
    "    if new_personas_processed % 5 == 0:\n",
    "        save_json(personas_save_path, personas_data)\n",
    "            # break\n",
    "\n",
    "    # if new_personas_processed > 2:break\n",
    "    \n",
    "save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb1330-ea62-440d-9417-55bf40842277",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcec6da-608c-454c-b9af-2c3c2a301d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    persona_id = 'persona_010'\n",
    "    filtered_jobs_ids = get_jobs_by_target_domains(personas[persona_id].target_domains, jobs_map)\n",
    "    print(filtered_jobs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132d376-3d3f-469d-b555-16e44ddb0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    persona_id = 'persona_010'\n",
    "    filtered_jobs_ids = get_jobs_by_target_domains(personas[persona_id].target_domains, jobs_map)\n",
    "    print(filtered_jobs_ids)\n",
    "    hard_filtered_jobs_ids = hard_filter_jobs(personas[persona_id], filtered_jobs_ids, jobs_info, verbose=True)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(hard_filtered_jobs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039e8e0-5e3b-4a0d-a20f-0053a2de7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    persona_id = 'persona_011'\n",
    "\n",
    "    hard_filtered_jobs_ids, selected_jobs_ids, rationale = matching_agent_process_jobs(persona_id, verbose=True)\n",
    "\n",
    "    print(f\"hard_filtered_jobs_ids : {hard_filtered_jobs_ids}\")\n",
    "    print(f\"selected_jobs_ids : {selected_jobs_ids}\")\n",
    "    print(f\"rationale : {rationale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5856c-7cb0-4030-8fae-bb2139e9ce02",
   "metadata": {},
   "source": [
    "# redo Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809b8ca4-8480-46c3-a610-38e98542d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona persona_018 without job proposed\n",
      "filtered_jobs_ids : ['j50', 'j51', 'j52', 'j53', 'j54', 'j55', 'j56', 'j57', 'j58', 'j59']\n",
      "j50\n",
      "excluded because of location : Brasília - São Paulo\n",
      "j51\n",
      "Location OK\n",
      "j52\n",
      "Location OK\n",
      "j53\n",
      "excluded because of location : Belo Horizonte - São Paulo\n",
      "j54\n",
      "excluded because of location : Salvador - São Paulo\n",
      "j55\n",
      "excluded because of location : Porto Alegre - São Paulo\n",
      "j56\n",
      "excluded because of location : Curitiba - São Paulo\n",
      "j57\n",
      "Location OK\n",
      "j58\n",
      "excluded because of location : Curitiba - São Paulo\n",
      "j59\n",
      "Location OK\n",
      "hard_filtered_jobs_ids : ['j51', 'j52', 'j57', 'j59']\n",
      "\n",
      "You are a job advisor expert in matching jobs to candidate profiles.\n",
      "\n",
      "Your task is to decide which jobs directly or partially support the candidate's goals.\n",
      "\n",
      "Candidate goal:\n",
      "training if it helps with product development in paper or pulping processes\n",
      "\n",
      "You are given a list of jobs:\n",
      "Job with ID: j51\n",
      "- Title: Assistant – Fiber Design\n",
      "- Description: Supports fiber and paper operations by ensuring product quality and optimizing design, conducting assessments, and collaborating on fiber-based product development.\n",
      "\n",
      "\n",
      "Job with ID: j52\n",
      "- Title: Fiber Optic Intern\n",
      "- Description: Fiber Optic Intern: Contribute to research and development of fiber optic applications in paper manufacturing, ensuring sustainability and innovation in the industry.\n",
      "\n",
      "\n",
      "Job with ID: j57\n",
      "- Title: Paper Industry Operations Consultant\n",
      "- Description: Optimize paper manufacturing processes and identify improvement opportunities by collaborating with operations teams, contributing to product development, and enhancing operational efficiency in the fibers and paper industry.\n",
      "\n",
      "\n",
      "Job with ID: j59\n",
      "- Title: Junior Specialist – Paper Technology\n",
      "- Description: Junior Specialist – Paper Technology will support sustainable paper production by implementing technology solutions, ensuring environmental compliance, and optimizing waste processing workflows. The role involves analyzing production data, improving processes, and collaborating on technology development while maintaining regulatory documentation and contributing to sustainability reporting.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For each job, select it if it fully or partially contributes to the candidate's goal, even if it does not cover all aspects.\n",
      "Clearly explain how the job supports the goal, even if only partially.\n",
      "\n",
      "Return the list of job IDs you have selected and rationale.\n",
      "\n",
      "selected_jobs_ids : ['j51', 'j52', 'j57', 'j59']\n",
      "{'name': '', 'age': 25, 'location': 'São Paulo', 'recommendation_type': 'jobs_trainings', 'open_to_relocate_for_work': False, 'work_type_preference': 'onsite', 'target_domains': ['Fiber And Paper Manufacturing Operations'], 'education_level': 'Graduação', 'years_of_experience': 3, 'skills_domains': [], 'skills': {}, 'languages': ['Portuguese', 'English'], 'goals': 'training if it helps with product development in paper or pulping processes', 'hard_filtered_jobs_ids': ['j51', 'j52', 'j57', 'j59'], 'proposed_job_ids': ['j51', 'j52', 'j57', 'j59'], 'rationale': \"j51: This job directly supports the candidate's goal by involving collaboration on fiber-based product development, which is relevant to product development in paper or pulping processes.\\n\\nj52: This job partially supports the candidate's goal as it involves research and development of fiber optic applications in paper manufacturing, contributing to innovation in the industry.\\n\\nj57: This job directly supports the candidate's goal by focusing on optimizing paper manufacturing processes and contributing to product development in the fibers and paper industry.\\n\\nj59: This job partially supports the candidate's goal as it involves implementing technology solutions and improving processes in sustainable paper production, which can contribute to product development in paper or pulping processes.\"}\n"
     ]
    }
   ],
   "source": [
    "person_id = 'persona_07'\n",
    "\n",
    "filename = f\"{output_prefix}_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "persona = personas[person_id]\n",
    "personas_data[person_id] = initial_personas_data[person_id]\n",
    "persona_data = json.loads(initial_personas_data[person_id])\n",
    "\n",
    "if persona.recommendation_type == \"jobs_trainings\":\n",
    "    print(f\"persona {person_id} without job proposed\")\n",
    "    hard_filtered_jobs_ids, selected_jobs_ids, rationale = matching_agent_process_jobs(person_id, verbose=True)\n",
    "    persona_data['hard_filtered_jobs_ids'] = hard_filtered_jobs_ids\n",
    "    persona_data['proposed_job_ids'] = selected_jobs_ids\n",
    "    persona_data['rationale'] = rationale\n",
    "    personas_data[person_id] = json.dumps(persona_data)\n",
    "    print(persona_data)\n",
    "\n",
    "save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62171ca2-0681-4baf-9045-9d3521417329",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4277482-e540-4e7a-abb6-5302ada9e2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 84682.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_041\n",
      "persona_048\n",
      "persona_069\n",
      "persona_077\n",
      "persona_079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = f\"{output_prefix}_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "for person_id in tqdm(personas):\n",
    "    persona = personas[person_id]\n",
    "    personas_data[person_id] = personas_data[person_id]\n",
    "    persona_data = json.loads(personas_data[person_id])\n",
    "    \n",
    "    if persona.recommendation_type == \"jobs_trainings\":\n",
    "        if 'proposed_job_ids' not in persona_data:\n",
    "            length = 0\n",
    "        else:\n",
    "            length = len(persona_data['proposed_job_ids'])\n",
    "        if length == 0:\n",
    "            print(person_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208dfad-3476-4fc2-ac13-c4efac1ccd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
