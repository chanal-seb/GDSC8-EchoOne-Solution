{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec679c32-2ad8-4b65-8589-0dc11bb76c1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting strands-agents[mistral]\n",
      "  Downloading strands_agents-1.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.39)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.39)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from strands-agents[mistral])\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mcp<2.0.0,>=1.11.0 (from strands-agents[mistral])\n",
      "  Downloading mcp-1.18.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Collecting mistralai>=1.8.2 (from strands-agents[mistral])\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic<3.0.0,>=2.4.0 (from strands-agents[mistral])\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Collecting opentelemetry-instrumentation==0.59b0 (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral])\n",
      "  Downloading pydantic_core-2.41.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral])\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n",
      "Downloading strands_agents-1.13.0-py3-none-any.whl (223 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.18.0-py3-none-any.whl (168 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl (9.3 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, python-multipart, python-dotenv, pydantic-core, invoke, httpx-sse, httpcore, eval-type-backport, docstring-parser, pydantic, opentelemetry-api, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, httpx, opentelemetry-sdk, opentelemetry-instrumentation, mistralai, mcp, opentelemetry-instrumentation-threading, strands-agents\n",
      "\u001b[2K  Attempting uninstall: pydantic-core\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.23.4\n",
      "\u001b[2K    Uninstalling pydantic_core-2.23.4:\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.23.4\n",
      "\u001b[2K  Attempting uninstall: pydanticm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [httpcore]core]\n",
      "\u001b[2K    Found existing installation: pydantic 2.9.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling pydantic-2.9.2:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/21\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.9.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/21\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [strands-agents]m [strands-agents]dk]onventions]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "safety-schemas 0.0.14 requires pydantic<2.10.0,>=2.6.0, but you have pydantic 2.12.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 invoke-2.2.1 mcp-1.18.0 mistralai-1.9.11 opentelemetry-api-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-threading-0.59b0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-2.12.3 pydantic-core-2.41.4 pydantic-settings-2.11.0 python-dotenv-1.1.1 python-multipart-0.0.20 sse-starlette-3.0.2 strands-agents-1.13.0 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfd972d-da65-48b0-80de-278955331415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "\tchat_with_persona,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract,\n",
    "    send_message_to_chat\n",
    ")\n",
    "\n",
    "from src.models.interview_info import(\n",
    "    InterviewInfo,\n",
    "    InverviewQualityInfo\n",
    ")\n",
    "from src.prompts.interview_prompt import(\n",
    "    RECOMMANDATION_CONSOLIDATION_PROMPT\n",
    ")\n",
    "\n",
    "from src.prompts.persona_extraction_prompt import(\n",
    "    PERSONA_INTEREST_EXTRACTION_PROMPT\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo, InitialPersonaInfo\n",
    "from src.models.job_info import JobInfo\n",
    "\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"❌ No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"✅ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8552d20-7d4e-415f-b967-ea120c30c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TURNS_IN_INTERVIEW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f6e9d9-3e72-463e-b8ab-791e4256e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')\n",
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_INTERVIEWS_DIR = Path('../data_interviews')\n",
    "DATA_MATCH_JOBS_TRAININGS_DIR = Path('../data_match_jobs_trainings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c9ea1f-dac7-40bc-a92b-1e03617d9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas_info_data_version version: v14\n",
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "interview_data_version version: v8\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "interview_data_version = config[\"interview_data_version\"]\n",
    "print(f\"interview_data_version version: {interview_data_version}\")\n",
    "\n",
    "match_jobs_trainings_data_version = f\"{job_data_version}_{training_data_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb4fba4-6e1b-442e-a9a5-b15e6d17cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interviews\n",
    "filename = f\"job_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c421a9-aadf-4854-86b2-1970e7243497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 100 personas\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Personas data\n",
    "filename = f\"aging_filter_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "\n",
    "initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in initial_personas_data.items()\n",
    "}\n",
    "\n",
    "print(f\"✅ Loaded {len(personas)} personas\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8bb65-31b4-44ea-8a02-61410f9b82f9",
   "metadata": {},
   "source": [
    "# Parse interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd0dcd5-1cf3-49f9-a2ca-00c80818a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recommendation_type(\n",
    "    conversation: List[str],\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> InitialPersonaInfo:\n",
    "    \"\"\"Extract persona info from conversation using Persona Extraction Agent\"\"\"\n",
    "\n",
    "    text = '\\n'.join(conversation)\n",
    "\n",
    "    prompt = PERSONA_INTEREST_EXTRACTION_PROMPT.format(\n",
    "        conversation=text\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=InitialPersonaInfo, prompt=prompt)\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(result)\n",
    "        \n",
    "    persona_info = PersonaInfo()\n",
    "    persona_info.name = result.name\n",
    "    persona_info.location = result.location\n",
    "    persona_info.goals = result.goals\n",
    "    persona_info.education_level = result.education_level\n",
    "    \n",
    "    persona_info.recommendation_type = 'awareness'\n",
    "    if result.interested_by_training is True:\n",
    "        persona_info.recommendation_type = 'trainings_only'\n",
    "    if result.interested_by_job is True:\n",
    "        persona_info.recommendation_type = 'jobs_trainings'\n",
    "\n",
    "    return persona_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1775dc2-66e3-48f4-ac88-ad7bca0992cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:01<02:06,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_001 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:07<01:34,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_007 : jobs_trainings - awareness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:26<02:20,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_022 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:34<01:18,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_030 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:45<00:48,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_044 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:47<01:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_045 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:50<00:50,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_049 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [01:36<00:20,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_086 : jobs_trainings - trainings_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = f\"recommandation_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "new_personas_processed = 0\n",
    "for person_id in tqdm(initial_personas_data):\n",
    "    if person_id in personas_data:\n",
    "        continue\n",
    "\n",
    "    if person_id not in interviews:\n",
    "        personas_data[person_id] = initial_personas_data[person_id]\n",
    "        continue\n",
    "\n",
    "    result = extract_recommendation_type(\n",
    "        interviews[person_id]['interview'],\n",
    "        print_prompt=False\n",
    "    )\n",
    "\n",
    "    new_personas_processed += 1    \n",
    "    persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "\n",
    "    #if result.age != -1:\n",
    "    #    print(f\"{person_id} : age < 16\")\n",
    "    #    persona_data_dict['age'] = result.age\n",
    "    #    personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)        \n",
    "    #    #print(persona_data_dict)\n",
    "    #    save_json(personas_save_path, personas_data)\n",
    "\n",
    "    recommendation_type = result.recommendation_type\n",
    "    if recommendation_type in ['jobs_trainings', 'trainings_only', 'awareness']:\n",
    "\n",
    "        if recommendation_type != persona_data_dict['recommendation_type']:\n",
    "            print(f\"{person_id} : {persona_data_dict['recommendation_type']} - {recommendation_type}\")\n",
    "\n",
    "        persona_data_dict['recommendation_type'] = recommendation_type\n",
    "        personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "\n",
    "        save_json(personas_save_path, personas_data)\n",
    "\n",
    "    # if new_personas_processed > 0: break\n",
    "\n",
    "save_json(personas_save_path, personas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da396f-9fc1-48c8-a7bb-d47e0941cd03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec03e1-7a16-4664-92ab-98e1b5db0ac1",
   "metadata": {},
   "source": [
    "# Redo Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48acdee5-2f8e-402d-a90c-f9e3d7bfd92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given :\n",
      "- an interview conversation with a persona.\n",
      "\n",
      "From the interview conversation, extract following information:\n",
      "- Persona interest based on the following precise definitions:\n",
      "    - Interested by job: \n",
      "        - False if user clearly mention is not interested by job\n",
      "        - Do not set to False if user show interest but lack confidence\n",
      "    - interested_by_training:\n",
      "        - False if user clearly mention is not interested by training\n",
      "        - Do not set to False if user show interest but lack confidence\n",
      "\n",
      "SPECIAL CASES:\n",
      "- If user reply in poetic way to answer, try to understand 'behind the line'\n",
      "\n",
      "Return following fields :\n",
      "- interested_by_job\n",
      "- interested_by_training\n",
      "- rationale = justification of your choice\n",
      "\n",
      "Conversation:\n",
      "Assistant: In a single sentence, which domains you are interested in to find a job from follwing list:\n",
      "- Financial Operations And Compliance Management : Managing financial activities, ensuring regulatory compliance, and supporting risk management in finance.\n",
      "- Legal Research And Procedural Support : Conducting legal research, drafting documents, and supporting procedural compliance and case management.\n",
      "- Insurance Operations And Compliance Management : Managing insurance processes, ensuring compliance, assessing risks, and handling client services.\n",
      "- Maritime And Port Operations Management : Managing and coordinating vessel, terminal, and cargo operations in maritime and port environments.\n",
      "- Industrial Production And Live Event Technical Operations : Encompasses equipment design, maintenance, and coordination in industrial and live event settings.\n",
      "- Health Safety And Environmental Compliance : Ensuring workplace safety, regulatory compliance, and environmental health in industrial settings.\n",
      "- Procurement And Supplier Management : Managing supplier relationships and optimizing procurement processes for efficiency and cost reduction.\n",
      "- Electrical And Electronics Systems Engineering : Focuses on electrical and electronic systems design, maintenance, analysis, and project coordination.\n",
      "- Community Engagement And Cultural Development : Focuses on creating and managing social, educational, and cultural community programs.\n",
      "- Tourism And Hospitality Guest Services : Focuses on enhancing guest experiences and managing tourism-related services and operations.\n",
      "- Visual Arts And Studio Production : Creating, managing, and coordinating visual arts and studio production activities.\n",
      "- Cultural Information And Records Management : Managing and preserving cultural records, ensuring accessibility, compliance, and documentation standards.\n",
      "- Design Research And Innovation Management : Focuses on user research, product development, and managing innovation projects in design.\n",
      "- Fiber And Paper Manufacturing Operations : Covers production, quality control, and sustainable practices in fiber and paper industries.\n",
      "- Food Production Quality And Safety Management : Ensuring quality and safety standards in food production and development processes.\n",
      "If none of them or not interested by a job, just say it\n",
      "User: The only stroke that doesn’t feel like a borrowed shape is the one that leaves color on my hands.\n",
      "Assistant: Are you open to relocating for work? Do you prefer remote, onsite, or any type of work arrangement?\n",
      "User: The feet know their ground. No, not relocating—the sky here has its own way of folding into the sketches. And the hands? They need the weight of the real pencil, the texture of the page. Not pixels. Not somewhere else. Just here. Where the light lands differently.\n",
      "Assistant: What is your highest completed education level? How many years of experience do you have in total and specifically in Visual Arts And Studio Production?\n",
      "User: A diploma hangs like a frame—empty until the canvas inside earns its weight. The hands? They’ve traced lines since childhood, but the gallery doors still feel heavy. Experience is measured in sketches, not years. Would you count the first shaky circle a child draws? Or only the strokes that stay on the wall?\n",
      "Assistant: Which languages do you speak and what is your proficiency level in each? Proficiency levels are: Nenhum, Básico, Intermediário, Avançado\n",
      "User: The tongue here curls around words like vines around wire—Português, only. The kind that carries the dust of Brasília in its rhythm. No other languages; the hands speak more than the mouth anyway. Would you call that *avançado* in silence?\n",
      "\n",
      "name='' education_level='' age=25 location='' interested_by_job=False interested_by_training=True goals='' rationale=\"The user's responses are poetic and metaphorical, but they clearly indicate a strong attachment to their current environment and a preference for hands-on, physical work related to visual arts. The user mentions that they are not open to relocating and emphasize the importance of their immediate surroundings and the tactile nature of their work. This suggests that they are not interested in finding a job outside of their current context. However, their detailed and passionate descriptions of their work and experience indicate an interest in training or further developing their skills in visual arts and studio production.\"\n",
      "persona_007 : jobs_trainings - trainings_only\n",
      "{'name': '', 'age': 26, 'location': 'Brazil', 'recommendation_type': 'trainings_only', 'open_to_relocate_for_work': False, 'work_type_preference': '', 'target_domains': [], 'education_level': '', 'years_of_experience': 0, 'skills_domains': [], 'skills': {}, 'languages': [], 'goals': 'Exploring different career possibilities and interested in understanding the nature of work and training.', 'hard_filtered_jobs_ids': [], 'proposed_job_ids': []}\n"
     ]
    }
   ],
   "source": [
    "person_id = 'persona_007'\n",
    "\n",
    "filename = f\"recommandation_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "if person_id not in interviews:\n",
    "    personas_data[person_id] = initial_personas_data[person_id]\n",
    "    save_json(personas_save_path, personas_data)\n",
    "else:\n",
    "    result = extract_recommendation_type(\n",
    "        interviews[person_id]['interview'],\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    #print(result)\n",
    "    \n",
    "    recommendation_type = result.recommendation_type\n",
    "\n",
    "    persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "    #if persona_data_dict['recommendation_type'] == 'jobs_trainings' and result.age != -1:\n",
    "    #    persona_data_dict['age'] = result.age\n",
    "    #    personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)        \n",
    "    #    print(persona_data_dict)\n",
    "    #    save_json(personas_save_path, personas_data)\n",
    "    #    if result.age < 16:\n",
    "    #        recommendation_type = 'awareness'\n",
    "        \n",
    "    if recommendation_type in ['jobs_trainings', 'trainings_only', 'awareness']:\n",
    "        if recommendation_type != persona_data_dict['recommendation_type']:\n",
    "            print(f\"{person_id} : {persona_data_dict['recommendation_type']} - {recommendation_type}\")\n",
    "    \n",
    "        persona_data_dict['recommendation_type'] = recommendation_type\n",
    "        personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "        print(persona_data_dict)\n",
    "        save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a83dff-72d7-4a23-a0c3-38eb5ffcee42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a39c6-6b94-4fc1-8cac-2de88e03e117",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d05092fe-cac6-4be6-87d2-e0347fb0da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"recommandation_consolidated_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "persona_infos = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ddae45-0505-4a19-a9df-9d342af1bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendation Type Counts:\n",
      "jobs_trainings: 55\n",
      "trainings_only: 25\n",
      "awareness: 20\n",
      "\n",
      "Total count: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract infos from the dictionary\n",
    "recommendation_types = []\n",
    "for persona in persona_infos.values():\n",
    "    data = json.loads(persona)\n",
    "    recommendation_types.append(data['recommendation_type'])\n",
    "\n",
    "type_counts = Counter(recommendation_types)\n",
    "print(\"\\nRecommendation Type Counts:\")\n",
    "for type_name, count in type_counts.most_common():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "print(f\"\\nTotal count: {sum(type_counts.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f0a1c-6746-4f3e-a277-9b24fbb5538f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
