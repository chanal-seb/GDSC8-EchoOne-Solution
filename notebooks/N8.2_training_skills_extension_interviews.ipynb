{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec679c32-2ad8-4b65-8589-0dc11bb76c1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting strands-agents[mistral]\n",
      "  Downloading strands_agents-1.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from strands-agents[mistral])\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mcp<2.0.0,>=1.11.0 (from strands-agents[mistral])\n",
      "  Downloading mcp-1.19.0-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Collecting mistralai>=1.8.2 (from strands-agents[mistral])\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Collecting opentelemetry-instrumentation==0.59b0 (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n",
      "Downloading strands_agents-1.13.0-py3-none-any.whl (223 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.19.0-py3-none-any.whl (170 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl (9.3 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: python-multipart, python-dotenv, invoke, httpx-sse, httpcore, eval-type-backport, docstring-parser, opentelemetry-api, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, httpx, opentelemetry-sdk, opentelemetry-instrumentation, mistralai, mcp, opentelemetry-instrumentation-threading, strands-agents\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18/18\u001b[0m [strands-agents]m [strands-agents]gs]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 invoke-2.2.1 mcp-1.19.0 mistralai-1.9.11 opentelemetry-api-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-threading-0.59b0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-settings-2.11.0 python-dotenv-1.2.1 python-multipart-0.0.20 sse-starlette-3.0.2 strands-agents-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfd972d-da65-48b0-80de-278955331415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "\tchat_with_persona,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract,\n",
    "    send_message_to_chat\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo, PersonaSkillsInterest\n",
    "from src.models.generic_models import ListOfStrs\n",
    "\n",
    "from src.models.interview_info import(\n",
    "    InterviewInfo,\n",
    "    InverviewQualityInfo,\n",
    "    InterviewAgentMessage\n",
    ")\n",
    "from src.prompts.interview_prompt import(\n",
    "    TRAINING_SKILLS_EXTENSION_INTERVIEW_PROMPT,\n",
    "    TRAINING_SKILLS_INTERVIEW_QUALITY_CHECK_PROMPT\n",
    ")\n",
    "from src.prompts.persona_extraction_prompt import(\n",
    "    PERSONA_SKILL_DOMAINS_CLASSIFICATION_PROMPT,\n",
    "    PERSONA_SKILL_DOMAINS_CLASSIFICATION_PROMPT_ALT\n",
    ")\n",
    "\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ùå No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"‚úÖ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8552d20-7d4e-415f-b967-ea120c30c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TURNS_IN_INTERVIEW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f6e9d9-3e72-463e-b8ab-791e4256e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')\n",
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_INTERVIEWS_DIR = Path('../data_interviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c9ea1f-dac7-40bc-a92b-1e03617d9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas_info_data_version version: v14\n",
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "interview_data_version version: v8\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "interview_data_version = config[\"interview_data_version\"]\n",
    "print(f\"interview_data_version version: {interview_data_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e38fcd-1a0d-4960-82a9-ee9479b7a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"map_clusters_jobs_{job_data_version}.json\"\n",
    "save_path = DATA_JOBS_DIR / filename\n",
    "jobs_map = read_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d53eed6-3264-4dd1-bd54-b8e1c457780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 12 skills domains\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load skills domains data\n",
    "filename = f\"final_map_clusters_trainings_{training_data_version}.json\"\n",
    "save_path = DATA_TRAININGS_DIR / filename\n",
    "trainings_map = read_json(save_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(trainings_map)} skills domains\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64cac2d-4e0f-46c2-9b12-6d0a1d410ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 personas\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Personas data\n",
    "filename = f\"final_with_jobs_trainings_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "\n",
    "initial_personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in initial_personas_data.items()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(personas)} personas\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61f5d9a-06f1-485b-8b1e-227a29d7a19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 12 skills domains\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load skills domains data\n",
    "filename = f\"final_map_clusters_trainings_{training_data_version}.json\"\n",
    "save_path = DATA_TRAININGS_DIR / filename\n",
    "trainings_map = read_json(save_path)\n",
    "\n",
    "trainings_map_lower = {key.lower(): value for key, value in trainings_map.items()}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(trainings_map)} skills domains\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9aa224-d406-4d32-8ab0-3a822853378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial training extension interview\n",
    "filename = f\"training_domain_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "initial_interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5858d-1ee7-47f2-9c75-7fcf1ede19e2",
   "metadata": {},
   "source": [
    "# Extract skill domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66b3947f-ade6-4a9d-8716-e691b4f4715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skill_domain_info(\n",
    "    formatted_domains,\n",
    "    interview,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt: bool = False\n",
    ") -> PersonaSkillsInterest:\n",
    "\n",
    "    text = '\\n'.join(interview)\n",
    "    \n",
    "    prompt = PERSONA_SKILL_DOMAINS_CLASSIFICATION_PROMPT.format(\n",
    "        formatted_domains=formatted_domains,\n",
    "        conversation=text\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "    \n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=PersonaSkillsInterest, prompt=prompt)\n",
    "\n",
    "    if hasattr(extraction_agent, 'last_response'):\n",
    "        track_api_call(extraction_agent.last_response, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82a92371-c154-4423-93bc-72e58d224cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    person_id = 'persona_001'\n",
    "    \n",
    "    domains_str = \"\"\n",
    "    for domain in trainings_map:\n",
    "        domains_str += f\"- {domain}\" + \"\\n\"\n",
    "\n",
    "    conversation_id = initial_interviews[person_id]['conversation_id']\n",
    "    initial_interview = initial_interviews[person_id]['interview']\n",
    "    #print(conversation_id)\n",
    "    #print(initial_interview)\n",
    "    #print(domains_str)\n",
    "\n",
    "    result = extract_skill_domain_info(\n",
    "        domains_str,\n",
    "        initial_interview,\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "    print(result.list_of_strs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "208c9544-f61d-4988-9979-a488cf94a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 30/100 [00:21<00:43,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_030\n",
      "awareness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:54<00:34,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_062\n",
      "awareness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:59<00:04,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona_086\n",
      "awareness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:09<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "cache_period = 5\n",
    "\n",
    "new_personas_processed = 0\n",
    "for person_id in tqdm(initial_personas_data):\n",
    "    initial_persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "    if initial_persona_data_dict['recommendation_type'] != 'trainings_only':\n",
    "        personas_data[person_id] = initial_personas_data[person_id]\n",
    "        continue\n",
    "        \n",
    "    if person_id in personas_data:\n",
    "        continue\n",
    "\n",
    "    if person_id not in initial_interviews:\n",
    "        personas_data[person_id] = initial_personas_data[person_id]\n",
    "        continue\n",
    "\n",
    "    #print(person_id)\n",
    "    new_personas_processed += 1\n",
    "\n",
    "    domains_str = \"\"\n",
    "    for domain in trainings_map:\n",
    "        domains_str += f\"- {domain}\" + \"\\n\"\n",
    "\n",
    "    conversation_id = initial_interviews[person_id]['conversation_id']\n",
    "    initial_interview = initial_interviews[person_id]['interview']\n",
    "\n",
    "    result = extract_skill_domain_info(\n",
    "        domains_str,\n",
    "        initial_interview,\n",
    "        model=\"mistral-medium-latest\",\n",
    "        print_prompt=False\n",
    "    )\n",
    "\n",
    "    persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "    if result.interested_by_training is False:\n",
    "        print(person_id)\n",
    "        print('awareness')\n",
    "        persona_data_dict['recommendation_type'] = 'awareness'\n",
    "    else:\n",
    "        persona_data_dict['skills_domains'] = result.list_of_skills\n",
    "\n",
    "    personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "\n",
    "    if new_personas_processed % cache_period == 0:\n",
    "        save_json(personas_save_path, personas_data)\n",
    "\n",
    "    #if new_personas_processed > 0:break\n",
    "\n",
    "save_json(personas_save_path, personas_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bfb5a-0d81-4133-a4d1-d098670ebbb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7238a0d-cb28-4845-adb4-b0f5026d513f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 79754.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n",
      "FIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "cache_period = 5\n",
    "\n",
    "new_personas_processed = 0\n",
    "for person_id in tqdm(personas_data):\n",
    "    persona_data_dict = json.loads(personas_data[person_id])\n",
    "    if persona_data_dict['recommendation_type'] == 'trainings_only':\n",
    "        skill_domains = persona_data_dict['skills_domains']\n",
    "        for domain in skill_domains:\n",
    "            if domain.lower() not in trainings_map_lower:\n",
    "                print(f\"{person_id} - {domain} not in trainings_map\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a7881-b99e-43dd-a38b-dfdd89c4336d",
   "metadata": {},
   "source": [
    "# Redo extract skill domain for persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92260978-fe01-4a6f-b3e7-bb19a68ea5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '', 'age': 16, 'location': 'Recife', 'recommendation_type': 'trainings_only', 'open_to_relocate_for_work': False, 'work_type_preference': '', 'target_domains': [], 'education_level': '', 'years_of_experience': 0, 'skills_domains': [], 'skills': {}, 'languages': [], 'goals': 'Understand career-related information, specifically about skills and what employers look for in the insurance industry.', 'hard_filtered_jobs_ids': [], 'proposed_job_ids': []}\n",
      "processing\n",
      "\n",
      "You are an expert in skill taxonomy, training analysis and candidate profile description analysis.\n",
      "\n",
      "Your task is to evaluate whether the candidate is interesting by any of the existing skill domains based on the interview conversation.\n",
      "\n",
      "# Existing Skill Domains:\n",
      "- Financial Risk Management And Compliance\n",
      "- Electrical And Electronic Systems Engineering\n",
      "- Food Safety And Management\n",
      "- Fiber And Paper Industry Operations\n",
      "- Industrial Equipment Maintenance And Optimization\n",
      "- Procurement And Supply Chain Management\n",
      "- Hospitality And Tourism Management\n",
      "- Legal Practice And Advocacy\n",
      "- Maritime And Port Operations Management\n",
      "- Visual And Artistic Skills\n",
      "- Information Management And Digital Security\n",
      "- Live Event Technical Management\n",
      "\n",
      "\n",
      "Each domain is represented as `'Domain Name'`.\n",
      "\n",
      "# Instructions:\n",
      "1. Carefully read the interview conversation and compare it to the domain list.\n",
      "2. Do not modify or rename existing domains. Especially, do not replace And by and or And by and.\n",
      "3. You are not authorized to create new skill domains.\n",
      "- You MUST use only the domains listed above.\n",
      "- Do NOT infer or map user interests to domains not explicitly listed.\n",
      "4.Return a Python object of class ListOfStrs with:\n",
      "- list_of_skills = List of skills persona shows interest in.\n",
      "- rationale = justification of your decision\n",
      "- interested_by_training = True by default\n",
      "5.If the user explicitly states they are not interested in any of the listed trainings and expresses no interest in training in any other domain, return:\n",
      "- interested_by_training = False\n",
      "6.If the user explicitly states they are unsure about training in any of the listed domains and expresses no interest in training in any other domain, return:\n",
      "- interested_by_training = False\n",
      "7.If the user expresses interest in training outside the provided list, return:\n",
      "- interested_by_training = True\n",
      "\n",
      "Conversation:\n",
      "Assistant: In a single sentence, which domains you are interested in to find trainings from following list:\n",
      "- Financial Risk Management And Compliance\n",
      "- Electrical And Electronic Systems Engineering\n",
      "- Food Safety And Management\n",
      "- Fiber And Paper Industry Operations\n",
      "- Industrial Equipment Maintenance And Optimization\n",
      "- Procurement And Supply Chain Management\n",
      "- Hospitality And Tourism Management\n",
      "- Legal Practice And Advocacy\n",
      "- Maritime And Port Operations Management\n",
      "- Visual And Artistic Skills\n",
      "- Information Management And Digital Security\n",
      "- Live Event Technical Management\n",
      "If none of them or not interested by a training, just say it\n",
      "User: No trainings, I just want information.\n",
      "\n",
      "list_of_skills=[] interested_by_training=False rationale='The user explicitly stated that they are not interested in any trainings and only want information.'\n",
      "{'name': '', 'age': 16, 'location': 'Recife', 'recommendation_type': 'awareness', 'open_to_relocate_for_work': False, 'work_type_preference': '', 'target_domains': [], 'education_level': '', 'years_of_experience': 0, 'skills_domains': [], 'skills': {}, 'languages': [], 'goals': 'Understand career-related information, specifically about skills and what employers look for in the insurance industry.', 'hard_filtered_jobs_ids': [], 'proposed_job_ids': []}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    person_id = 'persona_036'\n",
    "\n",
    "    filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "    personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "    personas_data = read_json(personas_save_path)\n",
    "\n",
    "    initial_persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "    print(initial_persona_data_dict)\n",
    "    if initial_persona_data_dict['recommendation_type'] != 'trainings_only':\n",
    "        print(\"Not training only\")\n",
    "        personas_data[person_id] = initial_personas_data[person_id]\n",
    "        print(personas_data[person_id])\n",
    "        save_json(personas_save_path, personas_data)\n",
    "    elif person_id not in initial_interviews:\n",
    "        print(\"Interview missing\")\n",
    "        #persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "        #persona_data_dict['recommendation_type'] = 'awareness'\n",
    "        #save_json(personas_save_path, personas_data)\n",
    "    else:\n",
    "        print(\"processing\")\n",
    "        domains_str = \"\"\n",
    "        for domain in trainings_map:\n",
    "            domains_str += f\"- {domain}\" + \"\\n\"\n",
    "    \n",
    "        conversation_id = initial_interviews[person_id]['conversation_id']\n",
    "        initial_interview = initial_interviews[person_id]['interview']\n",
    "    \n",
    "        result = extract_skill_domain_info(\n",
    "            domains_str,\n",
    "            initial_interview,\n",
    "            model=\"mistral-medium-latest\",\n",
    "            print_prompt=True\n",
    "        )\n",
    "        print(result)\n",
    "\n",
    "        persona_data_dict = json.loads(initial_personas_data[person_id])\n",
    "        if result.interested_by_training is False:\n",
    "            persona_data_dict['recommendation_type'] = 'awareness'\n",
    "        else:\n",
    "            persona_data_dict['skills_domains'] = result.list_of_skills\n",
    "            #persona_data_dict['skills_domains'] = ['Financial Risk Management And Compliance']\n",
    "\n",
    "        print(persona_data_dict)\n",
    "        \n",
    "        personas_data[person_id] = json.dumps(persona_data_dict, ensure_ascii=False)\n",
    "                \n",
    "        save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9d93d-62e0-4eb2-bdb6-0c7d96c6ecbe",
   "metadata": {},
   "source": [
    "# Skills interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45cbae59-f0a6-4df5-b941-f792ee870885",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a23b39-e849-44f3-ab02-0356c6ca0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_persona_interview(\n",
    "    persona_id: str,\n",
    "    skills_str: str,\n",
    "    confirm_age: bool = False,\n",
    "    conversation_id: str = None,\n",
    "    max_turns: int = 5,\n",
    "    model: str = \"mistral-medium-latest\",\n",
    "    print_conversation: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"Interview a persona and return conversation transcript\"\"\"\n",
    "\n",
    "    interview = InterviewInfo()\n",
    "    \n",
    "    # prompt = INITIAL_INTERVIEW_PROMPT\n",
    "    prompt = TRAINING_SKILLS_EXTENSION_INTERVIEW_PROMPT\n",
    "    \n",
    "    interview_agent = get_agent(prompt, model_id=model)\n",
    "\n",
    "    # Start with greeting\n",
    "    agent_message = \"From following list of skills, for which you are interested in and what is your current proficiency level (None, Basic, Intermediate, Advanced):\\n\"\n",
    "    agent_message += skills_str\n",
    "    agent_message += \"If none of them or not interested by a training, just say it\"\n",
    "\n",
    "    #print(conversation_id)\n",
    "    #print(prompt)\n",
    "    #print(agent_message)\n",
    "    #return None\n",
    "    \n",
    "    if print_conversation:\n",
    "        print(f\"Assistant: {agent_message}\")\n",
    "    interview.interview.append(f\"Assistant: {agent_message}\")\n",
    "\n",
    "    # Conduct interview\n",
    "    for turn in range(max_turns):\n",
    "        resp = send_message_to_chat(agent_message, persona_id, conversation_id)\n",
    "\n",
    "        if resp is None:\n",
    "            break\n",
    "\n",
    "        user_response, conversation_id = resp\n",
    "        interview.conversation_id = conversation_id\n",
    "        interview.interview.append(f\"User: {user_response}\")\n",
    "        if print_conversation:\n",
    "            print(f\"User: {user_response}\")\n",
    "\n",
    "        # Generate next question\n",
    "        conversation_str = '\\n'.join(interview.interview)\n",
    "        agent_response = interview_agent.structured_output(output_model=InterviewAgentMessage, prompt=conversation_str)\n",
    "        # agent_response = interview_agent.structured_output(output_model=InterviewAgentMessage, user_response)\n",
    "\n",
    "        # Track cost (using utils.py function)\n",
    "        # track_api_call(agent_response, model)\n",
    "\n",
    "        if agent_response.conversation_finished is True:\n",
    "            break\n",
    "            \n",
    "        agent_message = agent_response.message\n",
    "        interview.interview.append(f\"Assistant: {agent_message}\")\n",
    "        if print_conversation:\n",
    "            print(f\"Assistant: {agent_message}\")\n",
    "\n",
    "    return interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7112475f-99e0-4d79-8cd4-24fe54f8fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:09<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Interview all personas\n",
    "#persona_ids = [f'persona_{i:03}' for i in range(1, 4)]\n",
    "persona_ids = [f'persona_{i:03}' for i in range(1, 101)]\n",
    "\n",
    "# personas_save_path = SUBMISSION_DIR / 'personas.json'\n",
    "filename = f\"training_skills_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "if not interviews_save_path.exists():\n",
    "    save_json(interviews_save_path, {})\n",
    "interviews = read_json(interviews_save_path)\n",
    "\n",
    "# Track how many new personas we process\n",
    "new_personas_processed = 0\n",
    "\n",
    "for persona_id in tqdm(initial_interviews):\n",
    "\n",
    "    if persona_id in interviews:\n",
    "        continue\n",
    "\n",
    "    persona_info = personas[persona_id]\n",
    "\n",
    "    conversation_id = initial_interviews[persona_id]['conversation_id']\n",
    "\n",
    "    new_personas_processed += 1\n",
    "\n",
    "    personas_data_dict = json.loads(personas_data[persona_id])\n",
    "    skill_domains = personas_data_dict['skills_domains']\n",
    "    \n",
    "    skills_str = \"\"\n",
    "    for domain in skill_domains:\n",
    "        for skill in trainings_map[domain]:\n",
    "            skills_str += f\"- {domain} : {skill}\" + \"\\n\"\n",
    "\n",
    "    # Interview\n",
    "    conversation = conduct_persona_interview(\n",
    "        persona_id,\n",
    "        skills_str,\n",
    "        conversation_id=conversation_id,\n",
    "        max_turns=6,\n",
    "        print_conversation=False)\n",
    "    interviews[persona_id] = conversation.model_dump()\n",
    "\n",
    "    # Save every interview\n",
    "    if len(interviews) % 1 == 0:\n",
    "        save_json(interviews_save_path, interviews)\n",
    "\n",
    "    # Show cost update every 20 personas\n",
    "    # if new_personas_processed > 0 and new_personas_processed % 20 == 0:\n",
    "    #     print(f\"\\nüí∞ Cost update after {new_personas_processed} new personas:\")\n",
    "    #     print_cost_summary()\n",
    "    #     print()\n",
    "\n",
    "    #if new_personas_processed > 4:break\n",
    "\n",
    "save_json(interviews_save_path, interviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b55a4b-3f2d-4e5a-9c1e-dd9f3e491284",
   "metadata": {},
   "source": [
    "# Redo persona interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90efa19e-02f4-4b88-9fe1-f128f0dcc87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: From following list of skills, for which you are interested in and what is your current proficiency level (None, Basic, Intermediate, Advanced):\n",
      "If none of them or not interested by a training, just say it\n",
      "User: I don‚Äôt know any of these yet, but I‚Äôd love to learn about machine operation or basic maintenance‚Äîmy level is none, but I‚Äôm really curious!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13884/2037082638.py:48: DeprecationWarning: Agent.structured_output method is deprecated. You should pass in `structured_output_model` directly into the agent invocation. see: https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/structured-output/\n",
      "  agent_response = interview_agent.structured_output(output_model=InterviewAgentMessage, prompt=conversation_str)\n"
     ]
    }
   ],
   "source": [
    "filename = f\"training_domain_classified_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "if True:\n",
    "    persona_id = 'persona_025'\n",
    "\n",
    "    #filename = f\"training_skills_extension_interviews_{interview_data_version}.json\"\n",
    "    #interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "    #interviews = read_json(interviews_save_path)\n",
    "    #conversation_id = None\n",
    "\n",
    "    filename = f\"full_interviews_{persona_id}.json\"\n",
    "    interview_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "    interview = read_json(interview_save_path)\n",
    "    conversation_id = interview['conversation_id']\n",
    "\n",
    "    personas_data_dict = json.loads(personas_data[persona_id])\n",
    "    skill_domains = personas_data_dict['skills_domains']\n",
    "\n",
    "    confirm_age = False\n",
    "    \n",
    "    personas_data_dict\n",
    "\n",
    "    #for domain in skill_domains:\n",
    "    #    if domain.lower() not in trainings_map_lower:\n",
    "    #        print(f\"{person_id} - {domain} not in trainings_map\")\n",
    "\n",
    "    \n",
    "    skills_str = \"\"\n",
    "    for domain in skill_domains:\n",
    "        #if domain.lower() not in trainings_map_lower:\n",
    "        for skill in trainings_map_lower[domain.lower()]:\n",
    "            skills_str += f\"- {domain} : {skill}\" + \"\\n\"\n",
    "\n",
    "    # Interview\n",
    "    conversation = conduct_persona_interview(\n",
    "        persona_id,\n",
    "        skills_str,\n",
    "        conversation_id=conversation_id,\n",
    "        max_turns=6,\n",
    "        print_conversation=True)\n",
    "    \n",
    "    new_interview = conversation.model_dump()\n",
    "    interview['interview'].extend(new_interview['interview'])\n",
    "    save_json(interview_save_path, interview)\n",
    "    \n",
    "    #interviews[persona_id] = conversation.model_dump()\n",
    "\n",
    "    # Save every interview\n",
    "    #save_json(interviews_save_path, interviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf847e69-4c71-4365-82b1-96e7dd735618",
   "metadata": {},
   "source": [
    "# Check interview qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9dde91c-a633-45b1-9533-0e5d5c9f9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interview_quality(\n",
    "    interview: str,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt: bool = False\n",
    ") -> InverviewQualityInfo:\n",
    "    prompt = TRAINING_SKILLS_INTERVIEW_QUALITY_CHECK_PROMPT.format(\n",
    "        interview=interview\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=InverviewQualityInfo, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb207343-cbf1-4866-995d-4a10d61d0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"training_skills_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ba1074-4158-4fa9-b794-c459272822e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:23<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_period = 5\n",
    "\n",
    "filename = f\"quality_trainings_skills_interviews_{interview_data_version}.json\"\n",
    "save_path = DATA_INTERVIEWS_DIR / filename\n",
    "if not save_path.exists():\n",
    "    save_json(save_path, {})\n",
    "quality_interviews = read_json(save_path)\n",
    "\n",
    "new_items_processed = 0\n",
    "for persona_id in tqdm(initial_personas_data):\n",
    "    persona_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "\n",
    "    if persona_data_dict['recommendation_type'] != 'trainings_only':\n",
    "        continue\n",
    "\n",
    "    if persona_id in quality_interviews:\n",
    "        quality = json.loads(quality_interviews[persona_id])\n",
    "        if quality['quality_level'] == 'OK':\n",
    "            continue\n",
    "\n",
    "    if persona_id not in interviews:\n",
    "        quality_data = {\n",
    "            'quality_level': 'NOK',\n",
    "            'rationale': 'interview missing'\n",
    "        }\n",
    "        quality = InverviewQualityInfo(**quality_data)\n",
    "        quality_str = json.dumps(quality.model_dump(), ensure_ascii=False)\n",
    "        quality_interviews[persona_id] = quality_str\n",
    "        save_json(save_path, quality_interviews)\n",
    "        continue\n",
    "    \n",
    "    new_items_processed = new_items_processed + 1\n",
    "\n",
    "    interview = interviews[persona_id]['interview']\n",
    "\n",
    "    interview_str = \"\\n\".join(interview)\n",
    "    # print(interview_str)\n",
    "\n",
    "    quality = check_interview_quality(\n",
    "        interview_str,\n",
    "        print_prompt=False)\n",
    "    quality_str = json.dumps(quality.model_dump(), ensure_ascii=False)\n",
    "\n",
    "    quality_interviews[persona_id] = quality_str\n",
    "\n",
    "    if new_items_processed % cache_period == 0:\n",
    "        save_json(save_path, quality_interviews)\n",
    "\n",
    "    # if new_items_processed > 0:break\n",
    "\n",
    "save_json(save_path, quality_interviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529ccf61-a664-4724-b6e4-f1a2c988e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"quality_trainings_skills_interviews_{interview_data_version}.json\"\n",
    "save_path = DATA_INTERVIEWS_DIR / filename\n",
    "quality_interviews = read_json(save_path)\n",
    "\n",
    "for persona_id in quality_interviews:\n",
    "    quality = json.loads(quality_interviews[persona_id])\n",
    "    if quality['quality_level'] != 'OK':\n",
    "        print(persona_id)\n",
    "        print(quality['rationale'])\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50b3dc-0bc4-4b74-a7c6-2cfea74d3f24",
   "metadata": {},
   "source": [
    "# Redo interview of a Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe28935f-3861-4c58-9282-04060a6eb093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Conduct Interview...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conduct_persona_interview' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Interview\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müé§ Conduct Interview...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_persona_interview\u001b[49m(persona_id, max_turns\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, print_conversation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(conversation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conduct_persona_interview' is not defined"
     ]
    }
   ],
   "source": [
    "persona_id = \"persona_098\"\n",
    "\n",
    "# Interview\n",
    "print(\"üé§ Conduct Interview...\")\n",
    "conversation = conduct_persona_interview(persona_id, max_turns=10, print_conversation=True)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e2eef5-73ca-436b-be95-e267b5b16dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m interviews_save_path \u001b[38;5;241m=\u001b[39m DATA_INTERVIEWS_DIR \u001b[38;5;241m/\u001b[39m filename\n\u001b[1;32m      3\u001b[0m interviews \u001b[38;5;241m=\u001b[39m read_json(interviews_save_path)\n\u001b[0;32m----> 4\u001b[0m interviews[persona_id] \u001b[38;5;241m=\u001b[39m \u001b[43mconversation\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m      5\u001b[0m save_json(interviews_save_path, interviews)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversation' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f\"job_extension_interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)\n",
    "interviews[persona_id] = conversation.model_dump()\n",
    "save_json(interviews_save_path, interviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59416f13-8eba-4004-907d-081bac4f843d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaef335b-47de-4657-8bf7-9aec70fedf51",
   "metadata": {},
   "source": [
    "# Translate interviews in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c804d-0058-4325-8b20-4f268397cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_interview(\n",
    "    interview,\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> InterviewInfo:\n",
    "\n",
    "    prompt = TRANSLATE_INTERVIEW_PROMPT.format(\n",
    "        interview=interview\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=InterviewInfo, prompt=prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0145f-c97b-4f09-9f0f-32e650186e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8be107-e323-473c-b187-3b4ae4086ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_period = 5\n",
    "\n",
    "filename = f\"en_interviews_{interview_data_version}.json\"\n",
    "save_path = DATA_INTERVIEWS_DIR / filename\n",
    "if not save_path.exists():\n",
    "    save_json(save_path, {})\n",
    "en_interviews = read_json(save_path)\n",
    "\n",
    "\n",
    "new_items_processed = 0\n",
    "for interview_id in tqdm(interviews):\n",
    "    if interview_id in en_interviews:\n",
    "        continue\n",
    "\n",
    "    new_items_processed = new_items_processed + 1\n",
    "    \n",
    "    interview = interviews[interview_id]\n",
    "\n",
    "    translated_interview = translate_interview(interview, print_prompt=False)\n",
    "    en_interviews[interview_id] = translated_interview.interview\n",
    "\n",
    "    if new_items_processed % cache_period == 0:\n",
    "        save_json(save_path, en_interviews)\n",
    "\n",
    "    # if new_items_processed > 4:break\n",
    "\n",
    "save_json(save_path, en_interviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bba8d5-dd70-4e6a-94f8-f6da4ea35b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
