{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aec46e5-52bf-4fcd-b868-63fbab04bf54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting strands-agents[mistral]\n",
      "  Downloading strands_agents-1.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.52)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from strands-agents[mistral])\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mcp<2.0.0,>=1.11.0 (from strands-agents[mistral])\n",
      "  Downloading mcp-1.19.0-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.30.0 (from strands-agents[mistral])\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Collecting mistralai>=1.8.2 (from strands-agents[mistral])\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.11.0)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting httpx>=0.27.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Collecting opentelemetry-instrumentation==0.59b0 (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral])\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral])\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.8.2->strands-agents[mistral])\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai>=1.8.2->strands-agents[mistral]) (6.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.3.0)\n",
      "Downloading strands_agents-1.13.0-py3-none-any.whl (223 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.19.0-py3-none-any.whl (170 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_instrumentation_threading-0.59b0-py3-none-any.whl (9.3 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: python-multipart, python-dotenv, invoke, httpx-sse, httpcore, eval-type-backport, docstring-parser, opentelemetry-api, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, httpx, opentelemetry-sdk, opentelemetry-instrumentation, mistralai, mcp, opentelemetry-instrumentation-threading, strands-agents\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18/18\u001b[0m [strands-agents]m [strands-agents]dk]onventions]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 invoke-2.2.1 mcp-1.19.0 mistralai-1.9.11 opentelemetry-api-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-threading-0.59b0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-settings-2.11.0 python-dotenv-1.2.1 python-multipart-0.0.20 sse-starlette-3.0.2 strands-agents-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install strands-agents[mistral] python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fec45b-aa58-44fb-827c-0175956f2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key found, we're ready to roll\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "import pprint\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "\tchat_with_persona,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "from src.my_utils import (\n",
    "    display_markdown_file,\n",
    "    call_mistral,\n",
    "    get_agent,\n",
    "    batch_extract,\n",
    "    compute_stat_for_multi_items\n",
    ")\n",
    "\n",
    "from src.models.persona_info import PersonaInfo, InitialPersonaInfo\n",
    "from src.models.activity_domain_info import ActivityDomainInfo, ListOfActivityDomains\n",
    "from src.models.skill_domain_info import SkillDomainInfo, ListSkillsDomains\n",
    "from src.prompts.persona_extraction_prompt import (\n",
    "    PERSONA_EXTRACTION_PROMPT,\n",
    "    PERSONA_INITIAL_EXTRACTION_PROMPT,\n",
    "    PERSONA_EXTEND_SKILL_DOMAIN_PROMPT,\n",
    "    PERSONA_SKILL_DOMAINS_CLASSIFICATION_PROMPT,\n",
    "    PERSONA_ACTIVITY_DOMAINS_CLASSIFICATION_PROMPT\n",
    ")\n",
    "\n",
    "# Load API key from .env file\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "# Check if we're good to go\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ùå No MISTRAL_API_KEY found!\")\n",
    "    print(\"Create an env file with your API key\")\n",
    "else:\n",
    "    print(\"‚úÖ API key found, we're ready to roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e41af00-3ed7-4d1f-8bd0-b16446186248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JOBS_DIR = Path('../data_jobs')\n",
    "DATA_TRAININGS_DIR = Path('../data_trainings')\n",
    "DATA_INTERVIEWS_DIR = Path('../data_interviews')\n",
    "DATA_ACTIVITIES_DOMAINS_DIR = Path('../data_activities_domains')\n",
    "DATA_SKILLS_DOMAINS_DIR = Path('../data_skills_domains')\n",
    "DATA_PERSONAS_INFO_DIR = Path('../data_personas_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b5960a-d12b-4af8-b533-ba27fed5f106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_data_version version: v4\n",
      "training_data_version version: v7\n",
      "interview_data_version version: v8\n",
      "activity_domains_version version: v4\n",
      "personas_info_data_version version: v14\n",
      "skill_domains_version version: v3\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "job_data_version = config[\"job_data_version\"]\n",
    "print(f\"job_data_version version: {job_data_version}\")\n",
    "\n",
    "training_data_version = config[\"training_data_version\"]\n",
    "print(f\"training_data_version version: {training_data_version}\")\n",
    "\n",
    "interview_data_version = config[\"interview_data_version\"]\n",
    "print(f\"interview_data_version version: {interview_data_version}\")\n",
    "\n",
    "activity_domains_version = config[\"activity_domains_version\"]\n",
    "print(f\"activity_domains_version version: {activity_domains_version}\")\n",
    "\n",
    "personas_info_data_version = config[\"personas_info_data_version\"]\n",
    "print(f\"personas_info_data_version version: {personas_info_data_version}\")\n",
    "\n",
    "skill_domains_version = config[\"skill_domains_version\"]\n",
    "print(f\"skill_domains_version version: {skill_domains_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06806ca7-5fe3-4fb0-a851-57938ac909c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jobs domains map data\n",
    "filename = f\"map_clusters_jobs_{job_data_version}.json\"\n",
    "save_path = DATA_JOBS_DIR / filename\n",
    "jobs_map = read_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb17430b-2038-4b5f-9397-fe62d8395a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 12 skills domains\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load skills domains map data\n",
    "filename = f\"final_map_clusters_trainings_{training_data_version}.json\"\n",
    "save_path = DATA_TRAININGS_DIR / filename\n",
    "trainings_map = read_json(save_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(trainings_map)} skills domains\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2fbd9a-250c-48b0-9d5d-3ed396b5b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interviews\n",
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ac186-2682-4435-a2f8-78f0ef04f69a",
   "metadata": {},
   "source": [
    "# Persona info parsing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af10406c-e2f9-4f05-a4d5-e84fcebf8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_initial_persona_info(\n",
    "    conversation: List[str],\n",
    "    model: str = \"mistral-small-latest\",\n",
    "    print_prompt=False\n",
    ") -> InitialPersonaInfo:\n",
    "    \"\"\"Extract persona info from conversation using Persona Extraction Agent\"\"\"\n",
    "    text = '\\n'.join(conversation)\n",
    "    #print(text)\n",
    "\n",
    "    prompt = PERSONA_INITIAL_EXTRACTION_PROMPT.format(\n",
    "        conversation=text\n",
    "    )\n",
    "\n",
    "    if print_prompt is True:\n",
    "        print(prompt)\n",
    "\n",
    "    # return None\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model, temperature=0.0)\n",
    "    result = extraction_agent.structured_output(output_model=InitialPersonaInfo, prompt=prompt)\n",
    "\n",
    "    persona_info = PersonaInfo()\n",
    "    persona_info.name = result.name\n",
    "    persona_info.age = result.age\n",
    "    persona_info.location = result.location\n",
    "    persona_info.goals = result.goals\n",
    "    \n",
    "    persona_info.recommendation_type = 'awareness'\n",
    "    if result.interested_by_training is True:\n",
    "        persona_info.recommendation_type = 'trainings_only'\n",
    "    if result.interested_by_job is True:\n",
    "        persona_info.recommendation_type = 'jobs_trainings'\n",
    "        \n",
    "    # Track cost\n",
    "    if hasattr(extraction_agent, 'last_response'):\n",
    "        track_api_call(extraction_agent.last_response, model)\n",
    "\n",
    "    return persona_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e945ced-292e-4a45-9173-8ee331b27eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START CLASSIFICATION LOOP\n",
      "ITERATE CLASSIFICATION LOOP\n",
      "Personas to process: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the interview conversation, extract the following fields and return a structured profile with these fields:\n",
      "- Age: The person's age as a number.\n",
      "    - if age is not given but you know user has below 16, set age = 10\n",
      "    - if age is not given but you know user has at least 16 or above, set age = 20\n",
      "- Location: The city where the person lives\n",
      "- Interested by job: True if the person expresses interest in employment, even if they are undecided or considering other options.\n",
      "- Interested by training: True if Person is interested to find a training. Even if not completely sure\n",
      "- Goals: Their stated career or learning objectives.\n",
      "    - Make sure to capture all meaningful informations that will help understand acticity domain or skills domain in relatiob to candidate profile or goal.\n",
      "\n",
      "If the person is exploring career paths like comparing job vs training, consider this as interest in job and training\n",
      "\n",
      "For any information not explicitly mentioned:\n",
      "- If they're just seeking information rather than specific jobs/training, capture this in goals\n",
      "\n",
      "Be precise and only extract information that's actually present or can be reasonably inferred. Don't invent details.\n",
      "\n",
      "Conversation:\n",
      "Assistant: Hello, I'm here to helping you navigate your career path.\n",
      "\n",
      "User: hi i want job in food place. im 21 and like to lern about food safe. can u help?\n",
      "Assistant: Great! Let's start. Do you want to find a job in Brazil or are you considering opportunities abroad? Also, are you interested in any specific training related to food safety?\n",
      "User: no no i stay in sao paulo only. i want trainig first. like... how to keep food clean? and the mashines? do they let studnts touh the mashines?\n",
      "Assistant: Thank you for the clarification. To summarize, you're interested in finding a job and training in S√£o Paulo, specifically in food safety and handling equipment. Is that correct? Also, do you have any prior experience or education in this field?\n",
      "User: yea sumting like that. i no have expirience but i lern fast. i see videos of food lines and its cool. i can lern fast if i see the food lines? i finish school but only the basik one. Ensino Fundamental.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:39<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "MAX_LOOPS = 1\n",
    "cache_period = 5\n",
    "\n",
    "# Prepare personas info\n",
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "new_items_processed = 0\n",
    "\n",
    "print(\"START CLASSIFICATION LOOP\")\n",
    "for i in range(MAX_LOOPS):\n",
    "    print(\"ITERATE CLASSIFICATION LOOP\")\n",
    "\n",
    "    filt_personas_ids = []\n",
    "    for persona_id in interviews:\n",
    "        if persona_id not in personas_data:\n",
    "            filt_personas_ids.append(persona_id)\n",
    "\n",
    "    print(f'Personas to process: {len(filt_personas_ids)}')\n",
    "\n",
    "    if len(filt_personas_ids) == 0:\n",
    "        break\n",
    "\n",
    "    # Reset cost tracker if starting fresh\n",
    "    # if len(persona_infos) == 0:\n",
    "    #     reset_cost_tracker()\n",
    "    #     print(\"üí∞ Starting fresh - cost tracker reset\")\n",
    "\n",
    "    # Track how many new personas we process\n",
    "    new_personas_processed = 0\n",
    "\n",
    "    for persona_id in tqdm(filt_personas_ids):\n",
    "        # Load Interview\n",
    "        if persona_id in interviews:\n",
    "            new_personas_processed += 1\n",
    "            conversation = interviews[persona_id]['interview']\n",
    "\n",
    "            print_prompt = False\n",
    "            if new_personas_processed == 1:\n",
    "               print_prompt = True\n",
    "\n",
    "            # Extract initial persona informations\n",
    "            persona_info = extract_initial_persona_info(\n",
    "                conversation,\n",
    "                model = \"mistral-small-latest\",\n",
    "                print_prompt=print_prompt\n",
    "                )\n",
    "            personas_data[persona_id] = persona_info.model_dump_json()\n",
    "\n",
    "            # Save every 5 personas\n",
    "            if new_personas_processed % 5 == 0:\n",
    "                save_json(personas_save_path, personas_data)\n",
    "\n",
    "            # Show cost update every 20 personas\n",
    "            # if new_personas_processed > 0 and new_personas_processed % 20 == 0:\n",
    "            #     print(f\"\\nüí∞ Cost update after {new_personas_processed} new personas:\")\n",
    "            #     print_cost_summary()\n",
    "            #     print()\n",
    "\n",
    "        # if new_personas_processed > 1:break\n",
    "\n",
    "save_json(personas_save_path, personas_data)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "# personas = {\n",
    "#     pid: PersonaInfo.model_validate_json(data)\n",
    "#     for pid, data in persona_infos.items()\n",
    "# }\n",
    "\n",
    "# print(f\"\\n‚úÖ Interviewed {len(personas)} personas total ({new_personas_processed} new)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e1f3f-ad2c-4cb7-9bf5-535ee6ef8bcf",
   "metadata": {},
   "source": [
    "# Patch personas with age under 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50aae678-0736-42f7-ad36-7f78c5d75e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Personas data\n",
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "initial_personas_data = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0063b7f3-6aac-49cf-b254-b5c10a79f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"aging_filter_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "save_json(personas_save_path, {})\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "for persona_id in initial_personas_data:\n",
    "    personas_data_dict = json.loads(initial_personas_data[persona_id])\n",
    "    if personas_data_dict['age'] < 16:\n",
    "        personas_data_dict['recommendation_type'] = 'awareness'\n",
    "\n",
    "    personas_data[persona_id] = json.dumps(personas_data_dict, ensure_ascii=False)\n",
    "\n",
    "save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264673c7-0bb7-4df4-afd4-2086a2c5974b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# For Debug Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81818ca9-4a50-43ff-a33e-c0f65c385aa9",
   "metadata": {},
   "source": [
    "# Redo persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e638678c-201f-40b3-8acb-62bef3ccf38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the interview conversation, extract the following fields and return a structured profile with these fields:\n",
      "- Age: The person's age as a number.\n",
      "    - if age is not given but you know user has below 16, set age = 10\n",
      "    - if age is not given but you know user has at least 16 or above, set age = 20\n",
      "- Location: The city where the person lives\n",
      "- Interested by job: True if the person expresses interest in employment, even if they are undecided or considering other options.\n",
      "- Interested by training: True if Person is interested to find a training. Even if not completely sure\n",
      "- Goals: Their stated career or learning objectives.\n",
      "    - Make sure to capture all meaningful informations that will help understand acticity domain or skills domain in relatiob to candidate profile or goal.\n",
      "\n",
      "If the person is exploring career paths like comparing job vs training, consider this as interest in job and training\n",
      "\n",
      "For any information not explicitly mentioned:\n",
      "- If they're just seeking information rather than specific jobs/training, capture this in goals\n",
      "\n",
      "Be precise and only extract information that's actually present or can be reasonably inferred. Don't invent details.\n",
      "\n",
      "Conversation:\n",
      "Assistant: Hello, I'm here to helping you navigate your career path.\n",
      "\n",
      "User: What is this tool? Is it for jobs or just information?\n",
      "Assistant: This tool is designed to assist you with various career-related needs. Are you looking to find a job in Brazil or abroad, explore training opportunities, or simply gather information? Also, could you please tell me your age or confirm if you are under 16 and the city where you currently live? It's important for us to provide the best guidance.\n",
      "User: I‚Äôm 16 and live in Recife. I don‚Äôt want a job yet, just to understand things. So do I answer questions here, or do I ask them?\n",
      "Assistant: You can ask questions and share information here. To better assist you, could you tell me what specific information you are looking for? For example, are you interested in learning about certain industries, job market trends, or educational paths?\n",
      "User: Does this tool also explain what skills are important in Insurance? Like, what employers look for?\n",
      "\n",
      "name='' age=16 location='Recife' recommendation_type='trainings_only' open_to_relocate_for_work=False work_type_preference='' target_domains=[] education_level='' years_of_experience=0 skills_domains=[] skills={} languages=[] goals='Understand career-related information, specifically about skills and what employers look for in the insurance industry.' hard_filtered_jobs_ids=[] proposed_job_ids=[]\n"
     ]
    }
   ],
   "source": [
    "persona_id = 'persona_062'\n",
    "conversation = interviews[persona_id]['interview']\n",
    "\n",
    "# Extract initial persona informations\n",
    "persona_info = extract_initial_persona_info(\n",
    "    conversation,\n",
    "    model = \"mistral-small-latest\",\n",
    "    print_prompt=True\n",
    "    )\n",
    "\n",
    "print(persona_info)\n",
    "# personas_data[persona_id] = persona_info.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13756c70-c256-49eb-8e18-33299adbc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "personas_data[persona_id] = persona_info.model_dump_json()\n",
    "save_json(personas_save_path, personas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99d409-a2ec-41a2-b44a-304fbf2da532",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636332e-f396-4945-8392-09629b29b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename = f\"interviews_{interview_data_version}.json\"\n",
    "    interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "    interviews = read_json(interviews_save_path)\n",
    "\n",
    "    persona_id = \"persona_011\"\n",
    "\n",
    "    conversation = interviews[persona_id]['interview']\n",
    "\n",
    "    print(conversation)\n",
    "\n",
    "    persona_info = extract_initial_persona_info(\n",
    "        conversation,\n",
    "        model=\"mistral-small-latest\",\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    print(persona_info)\n",
    "\n",
    "    # if(persona_info.recommendation_type != \"awareness\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934faec-121e-451c-a249-a2b905f6b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename = f\"interviews_{interview_data_version}.json\"\n",
    "    interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "    interviews = read_json(interviews_save_path)\n",
    "\n",
    "    persona_id = \"persona_011\"\n",
    "\n",
    "    conversation = interviews[persona_id]['interview']\n",
    "\n",
    "    print(conversation)\n",
    "\n",
    "    persona_info = extract_initial_persona_info(\n",
    "        conversation,\n",
    "        model=\"mistral-small-latest\",\n",
    "        print_prompt=True\n",
    "    )\n",
    "\n",
    "    print(persona_info)\n",
    "\n",
    "    # if(persona_info.recommendation_type != \"awareness\"):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fdc76-fe05-4795-a3e6-44bd8ecec3f4",
   "metadata": {},
   "source": [
    "# Persona info statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daac4b36-e1e1-4f63-9023-b032ff814cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"aging_filter_personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "persona_infos = read_json(personas_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be33d968-96ef-4033-bcc0-2f8a797a720d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 13: 2 occurrences\n",
      "Age 14: 1 occurrences\n",
      "Age 15: 5 occurrences\n",
      "Age 16: 10 occurrences\n",
      "Age 17: 3 occurrences\n",
      "Age 18: 12 occurrences\n",
      "Age 19: 7 occurrences\n",
      "Age 20: 3 occurrences\n",
      "Age 21: 7 occurrences\n",
      "Age 22: 7 occurrences\n",
      "Age 23: 4 occurrences\n",
      "Age 24: 4 occurrences\n",
      "Age 25: 6 occurrences\n",
      "Age 26: 8 occurrences\n",
      "Age 27: 9 occurrences\n",
      "Age 28: 7 occurrences\n",
      "Age 30: 2 occurrences\n",
      "\n",
      "Recommendation Type Counts:\n",
      "jobs_trainings: 61\n",
      "trainings_only: 21\n",
      "awareness: 18\n",
      "\n",
      "Total count: 100\n",
      "\n",
      "Location Counts:\n",
      "S√£o Paulo: 12\n",
      "Belo Horizonte: 12\n",
      "Recife: 11\n",
      "Bras√≠lia: 11\n",
      "Curitiba: 10\n",
      "Salvador: 8\n",
      "Porto Alegre: 8\n",
      "Rio: 6\n",
      ": 6\n",
      "Rio de Janeiro: 6\n",
      "Fortaleza: 4\n",
      "Brazil: 3\n",
      "Fortaleza, Brazil: 1\n",
      "Recife, Brazil: 1\n",
      "Salvador, Brazil: 1\n",
      "\n",
      "Total count: 100\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract ages from the JSON data\n",
    "ages = []\n",
    "for persona in persona_infos.values():\n",
    "    data = json.loads(persona)\n",
    "    if data['age'] > 0:  # Filter out invalid age (0)\n",
    "        ages.append(data['age'])\n",
    "age_counts = Counter(ages)\n",
    "# Print in ascending order of age\n",
    "for age in sorted(age_counts.keys()):\n",
    "    print(f\"Age {age}: {age_counts[age]} occurrences\")\n",
    "\n",
    "# Extract infos from the dictionary\n",
    "recommendation_types = []\n",
    "locations = []\n",
    "for persona in persona_infos.values():\n",
    "    data = json.loads(persona)\n",
    "    recommendation_types.append(data['recommendation_type'])\n",
    "    locations.append(data['location'])\n",
    "\n",
    "type_counts = Counter(recommendation_types)\n",
    "print(\"\\nRecommendation Type Counts:\")\n",
    "for type_name, count in type_counts.most_common():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "print(f\"\\nTotal count: {sum(type_counts.values())}\")\n",
    "\n",
    "type_counts = Counter(locations)\n",
    "print(\"\\nLocation Counts:\")\n",
    "for type_name, count in type_counts.most_common():\n",
    "    print(f\"{type_name}: {count}\")\n",
    "print(f\"\\nTotal count: {sum(type_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873b4a4-1490-4871-ae58-9d13060138cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19224f59-4df5-460d-a775-5e335fb97d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Testing Persona information Extraction Agent...\n",
      "Reading a sample job file...\n",
      "\n",
      "name='Camila' age=22 location='Fortaleza' recommendation_type='trainings_only' open_to_relocate_for_work=False work_type_preference='onsite' target_domains=['UNKNOWN'] education_level='T√©cnico' years_of_experience=0 skills_domains=['UNKNOWN'] skills={} languages={'Portuguese', 'English'} goals='Learn all basics of live event production: lights, sound, stage setup.'\n"
     ]
    }
   ],
   "source": [
    "# Correct Persona Info Extraction\n",
    "print(\"üíº Testing Persona information Extraction Agent...\")\n",
    "print(\"Reading a sample job file...\\n\")\n",
    "\n",
    "filename = f\"interviews_{interview_data_version}.json\"\n",
    "interviews_save_path = DATA_INTERVIEWS_DIR / filename\n",
    "interviews = read_json(interviews_save_path)\n",
    "\n",
    "filename = f\"personas_info_{personas_info_data_version}.json\"\n",
    "personas_save_path = DATA_PERSONAS_INFO_DIR / filename\n",
    "personas_data = read_json(personas_save_path)\n",
    "\n",
    "# Get first job file\n",
    "persona_id = \"persona_015\"\n",
    "\n",
    "if persona_id in interviews:\n",
    "    conversation = interviews[persona_id]\n",
    "\n",
    "    # Extract\n",
    "    persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "    personas_data[persona_id] = persona_info.model_dump_json()\n",
    "    save_json(personas_save_path, personas_data)\n",
    "\n",
    "    print(persona_info)\n",
    "    #persona_infos[persona_id] = persona_info.model_dump_json()\n",
    "    \n",
    "    # persona_info = extract_persona_info(activities_domains, skills_domains, conversation)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025f470-5693-451a-a5d3-08502b9162fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
